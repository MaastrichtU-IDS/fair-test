{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"fair-test is a Python library to build and deploy FAIR metrics tests APIs, supporting the specifications used by the FAIRMetrics working group , that can be queried to assess if a resource is complying with the FAIR principles (Findable, Accessible, Interoperable, Reusable). It aims to enable Python developers to easily write, and deploy FAIR metric tests functions that can be queried by various FAIR evaluations services, such as FAIR enough and the FAIRsharing FAIR Evaluator . FAIR metrics tests are evaluations taking a subject URL as input, executing a battery of tests (e.g. checking if machine readable metadata is available at this URL), and returning a score of 0 or 1, with the evaluation logs. \u2139\ufe0f How it works You define FAIR metric tests using custom python objects in separate files in a specific folder. The objects will guide you to provide all required metadata for your test as attributes, and implement the test evaluation logic as a function. The library also provides additional helper functions for common tasks, such as retrieving metadata from a URL, or testing a metric test. These tests can then be deployed as a web API, and registered in central FAIR evaluation service supporting the FAIR metrics working group framework, such as FAIR enough or the FAIR evaluator . Finally, users of the evaluation services will be able to group the registered metrics tests in collections used to assess the quality of publicly available digital objects. Report issues Feel free to create issues on GitHub , if you are facing problems, have a question, or would like to see a feature implemented. Pull requests are welcome! \ud83d\uddc3\ufe0f Projects using fair-test Here are some projects using fair-test to deploy FAIR test services: https://github.com/MaastrichtU-IDS/fair-enough-metrics : A generic FAIR metrics tests service developed at the Institute of Data Science at Maastricht University. https://github.com/LUMC-BioSemantics/RD-FAIRmetric-F4 : A FAIR metrics tests service for Rare Disease research. \ud83e\udd1d Credits Thanks to the people behind the FAIR evaluation services FAIR evaluator , F-UJI , and FAIR Checker , for the various inspirations and ideas. Thanks to the FAIR metrics working group for the specifications for FAIR evaluation services they defined. Library built with FastAPI , and RDFLib .","title":"About"},{"location":"#i-how-it-works","text":"You define FAIR metric tests using custom python objects in separate files in a specific folder. The objects will guide you to provide all required metadata for your test as attributes, and implement the test evaluation logic as a function. The library also provides additional helper functions for common tasks, such as retrieving metadata from a URL, or testing a metric test. These tests can then be deployed as a web API, and registered in central FAIR evaluation service supporting the FAIR metrics working group framework, such as FAIR enough or the FAIR evaluator . Finally, users of the evaluation services will be able to group the registered metrics tests in collections used to assess the quality of publicly available digital objects. Report issues Feel free to create issues on GitHub , if you are facing problems, have a question, or would like to see a feature implemented. Pull requests are welcome!","title":"\u2139\ufe0f How it works"},{"location":"#projects-using-fair-test","text":"Here are some projects using fair-test to deploy FAIR test services: https://github.com/MaastrichtU-IDS/fair-enough-metrics : A generic FAIR metrics tests service developed at the Institute of Data Science at Maastricht University. https://github.com/LUMC-BioSemantics/RD-FAIRmetric-F4 : A FAIR metrics tests service for Rare Disease research.","title":"\ud83d\uddc3\ufe0f Projects using fair-test"},{"location":"#credits","text":"Thanks to the people behind the FAIR evaluation services FAIR evaluator , F-UJI , and FAIR Checker , for the various inspirations and ideas. Thanks to the FAIR metrics working group for the specifications for FAIR evaluation services they defined. Library built with FastAPI , and RDFLib .","title":"\ud83e\udd1d Credits"},{"location":"FairTest/","text":"FairTest Bases: BaseModel Class to define a FAIR metrics test, API calls will be automatically generated for this test when the FairTestAPI is started. metrics/a1_check_something.py from fair_test import FairTest , FairTestEvaluation class MetricTest ( FairTest ): metric_path = 'a1-check-something' applies_to_principle = 'A1' title = 'Check something' description = \"Test something\" author = 'https://orcid.org/0000-0000-0000-0000' metric_version = '0.1.0' test_test = { 'http://doi.org/10.1594/PANGAEA.908011' : 1 , 'https://github.com/MaastrichtU-IDS/fair-test' : 0 , } def evaluate ( self , eval : FairTestEvaluation ): eval . info ( f 'Checking something for { self . subject } ' ) g = eval . retrieve_metadata ( self . subject , use_harvester = False ) if len ( g ) > 0 : eval . success ( f ' { len ( g ) } triples found, test sucessful' ) else : eval . failure ( 'No triples found, test failed' ) return eval . response () Source code in fair_test/fair_test.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class FairTest ( BaseModel ): \"\"\" Class to define a FAIR metrics test, API calls will be automatically generated for this test when the FairTestAPI is started. ```python title=\"metrics/a1_check_something.py\" from fair_test import FairTest, FairTestEvaluation class MetricTest(FairTest): metric_path = 'a1-check-something' applies_to_principle = 'A1' title = 'Check something' description = \"Test something\" author = 'https://orcid.org/0000-0000-0000-0000' metric_version = '0.1.0' test_test={ 'http://doi.org/10.1594/PANGAEA.908011': 1, 'https://github.com/MaastrichtU-IDS/fair-test': 0, } def evaluate(self, eval: FairTestEvaluation): eval.info(f'Checking something for {self.subject}') g = eval.retrieve_metadata(self.subject, use_harvester=False) if len(g) > 0: eval.success(f'{len(g)} triples found, test sucessful') else: eval.failure('No triples found, test failed') return eval.response() ``` \"\"\" # subject: Optional[str] # comment: List = [] # score: int = 0 # score_bonus: int = 0 # date: str = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S+01:00\") metric_version : str = \"0.1.0\" metric_path : str applies_to_principle : str id : Optional [ str ] # URL of the test results title : str description : str metric_readme_url : Optional [ str ] default_subject : str = settings . DEFAULT_SUBJECT topics = [] test_test = {} # topics: List[str] = [] # test_test: Dict[str, int] = {} author : str = settings . CONTACT_ORCID contact_url : str = settings . CONTACT_URL contact_name : str = settings . CONTACT_NAME contact_email : str = settings . CONTACT_EMAIL organization : str = settings . ORG_NAME # metric_readme_url: str = None def __init__ ( self ) -> None : super () . __init__ () if not self . metric_readme_url : self . metric_readme_url = f \" { settings . HOST_URL } /tests/ { self . metric_path } \" class Config : arbitrary_types_allowed = True def do_evaluate ( self , input : MetricInput ): if input . subject == \"\" : raise HTTPException ( status_code = 422 , detail = f \"Provide a subject URL to evaluate\" ) # TODO: create separate object for each FAIR test evaluation to avoid any conflict? e.g. FairTestEvaluation eval = FairTestEvaluation ( input . subject , self . metric_path ) # self.subject = input.subject return self . evaluate ( eval ) # try: # return self.evaluate(eval) # except Exception e: # return JSONResponse({ # 'errorMessage': f'Error while running the evaluation against {input.subject}' # }) # Placeholder that will be overwritten for each Metric Test def evaluate ( self , eval : FairTestEvaluation ): return JSONResponse ({ \"errorMessage\" : \"Not implemented\" }) # https://github.com/LUMC-BioSemantics/RD-FAIRmetrics/blob/main/docs/yaml/RD-R1.yml # Function used for the GET YAML call for infos about each Metric Test def openapi_yaml ( self ): metric_info = { \"swagger\" : \"2.0\" , \"info\" : { \"version\" : f \" { str ( self . metric_version ) } \" , \"title\" : self . title , \"x-tests_metric\" : self . metric_readme_url , \"description\" : self . description , \"x-applies_to_principle\" : self . applies_to_principle , \"x-topics\" : self . topics , \"contact\" : { \"x-organization\" : self . organization , \"url\" : self . contact_url , # \"name\": self.contact_name.encode('latin1').decode('iso-8859-1'), \"name\" : self . contact_name , \"x-role\" : \"responsible developer\" , \"email\" : self . contact_email , \"x-id\" : self . author , }, }, \"host\" : settings . HOST_URL . replace ( \"https://\" , \"\" ) . replace ( \"http://\" , \"\" ), \"basePath\" : \"/tests/\" , \"schemes\" : [ \"https\" ], \"paths\" : { self . metric_path : { \"post\" : { \"parameters\" : [ { \"name\" : \"content\" , \"in\" : \"body\" , \"required\" : True , \"schema\" : { \"$ref\" : \"#/definitions/schemas\" }, } ], \"consumes\" : [ \"application/json\" ], \"produces\" : [ \"application/json\" ], \"responses\" : { \"200\" : { \"description\" : \"The response is a binary (1/0), success or failure\" }}, } } }, \"definitions\" : { \"schemas\" : { \"required\" : [ \"subject\" ], \"properties\" : { \"subject\" : { \"type\" : \"string\" , \"description\" : \"the GUID being tested\" , } }, } }, } api_yaml = yaml . dump ( metric_info , indent = 2 , allow_unicode = True ) return PlainTextResponse ( content = api_yaml , media_type = \"text/x-yaml\" )","title":"<span><i class='fa-solid fa-flask'></i>&nbsp;&nbsp;FairTest</span>"},{"location":"FairTest/#fairtest","text":"Bases: BaseModel Class to define a FAIR metrics test, API calls will be automatically generated for this test when the FairTestAPI is started. metrics/a1_check_something.py from fair_test import FairTest , FairTestEvaluation class MetricTest ( FairTest ): metric_path = 'a1-check-something' applies_to_principle = 'A1' title = 'Check something' description = \"Test something\" author = 'https://orcid.org/0000-0000-0000-0000' metric_version = '0.1.0' test_test = { 'http://doi.org/10.1594/PANGAEA.908011' : 1 , 'https://github.com/MaastrichtU-IDS/fair-test' : 0 , } def evaluate ( self , eval : FairTestEvaluation ): eval . info ( f 'Checking something for { self . subject } ' ) g = eval . retrieve_metadata ( self . subject , use_harvester = False ) if len ( g ) > 0 : eval . success ( f ' { len ( g ) } triples found, test sucessful' ) else : eval . failure ( 'No triples found, test failed' ) return eval . response () Source code in fair_test/fair_test.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 class FairTest ( BaseModel ): \"\"\" Class to define a FAIR metrics test, API calls will be automatically generated for this test when the FairTestAPI is started. ```python title=\"metrics/a1_check_something.py\" from fair_test import FairTest, FairTestEvaluation class MetricTest(FairTest): metric_path = 'a1-check-something' applies_to_principle = 'A1' title = 'Check something' description = \"Test something\" author = 'https://orcid.org/0000-0000-0000-0000' metric_version = '0.1.0' test_test={ 'http://doi.org/10.1594/PANGAEA.908011': 1, 'https://github.com/MaastrichtU-IDS/fair-test': 0, } def evaluate(self, eval: FairTestEvaluation): eval.info(f'Checking something for {self.subject}') g = eval.retrieve_metadata(self.subject, use_harvester=False) if len(g) > 0: eval.success(f'{len(g)} triples found, test sucessful') else: eval.failure('No triples found, test failed') return eval.response() ``` \"\"\" # subject: Optional[str] # comment: List = [] # score: int = 0 # score_bonus: int = 0 # date: str = datetime.datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S+01:00\") metric_version : str = \"0.1.0\" metric_path : str applies_to_principle : str id : Optional [ str ] # URL of the test results title : str description : str metric_readme_url : Optional [ str ] default_subject : str = settings . DEFAULT_SUBJECT topics = [] test_test = {} # topics: List[str] = [] # test_test: Dict[str, int] = {} author : str = settings . CONTACT_ORCID contact_url : str = settings . CONTACT_URL contact_name : str = settings . CONTACT_NAME contact_email : str = settings . CONTACT_EMAIL organization : str = settings . ORG_NAME # metric_readme_url: str = None def __init__ ( self ) -> None : super () . __init__ () if not self . metric_readme_url : self . metric_readme_url = f \" { settings . HOST_URL } /tests/ { self . metric_path } \" class Config : arbitrary_types_allowed = True def do_evaluate ( self , input : MetricInput ): if input . subject == \"\" : raise HTTPException ( status_code = 422 , detail = f \"Provide a subject URL to evaluate\" ) # TODO: create separate object for each FAIR test evaluation to avoid any conflict? e.g. FairTestEvaluation eval = FairTestEvaluation ( input . subject , self . metric_path ) # self.subject = input.subject return self . evaluate ( eval ) # try: # return self.evaluate(eval) # except Exception e: # return JSONResponse({ # 'errorMessage': f'Error while running the evaluation against {input.subject}' # }) # Placeholder that will be overwritten for each Metric Test def evaluate ( self , eval : FairTestEvaluation ): return JSONResponse ({ \"errorMessage\" : \"Not implemented\" }) # https://github.com/LUMC-BioSemantics/RD-FAIRmetrics/blob/main/docs/yaml/RD-R1.yml # Function used for the GET YAML call for infos about each Metric Test def openapi_yaml ( self ): metric_info = { \"swagger\" : \"2.0\" , \"info\" : { \"version\" : f \" { str ( self . metric_version ) } \" , \"title\" : self . title , \"x-tests_metric\" : self . metric_readme_url , \"description\" : self . description , \"x-applies_to_principle\" : self . applies_to_principle , \"x-topics\" : self . topics , \"contact\" : { \"x-organization\" : self . organization , \"url\" : self . contact_url , # \"name\": self.contact_name.encode('latin1').decode('iso-8859-1'), \"name\" : self . contact_name , \"x-role\" : \"responsible developer\" , \"email\" : self . contact_email , \"x-id\" : self . author , }, }, \"host\" : settings . HOST_URL . replace ( \"https://\" , \"\" ) . replace ( \"http://\" , \"\" ), \"basePath\" : \"/tests/\" , \"schemes\" : [ \"https\" ], \"paths\" : { self . metric_path : { \"post\" : { \"parameters\" : [ { \"name\" : \"content\" , \"in\" : \"body\" , \"required\" : True , \"schema\" : { \"$ref\" : \"#/definitions/schemas\" }, } ], \"consumes\" : [ \"application/json\" ], \"produces\" : [ \"application/json\" ], \"responses\" : { \"200\" : { \"description\" : \"The response is a binary (1/0), success or failure\" }}, } } }, \"definitions\" : { \"schemas\" : { \"required\" : [ \"subject\" ], \"properties\" : { \"subject\" : { \"type\" : \"string\" , \"description\" : \"the GUID being tested\" , } }, } }, } api_yaml = yaml . dump ( metric_info , indent = 2 , allow_unicode = True ) return PlainTextResponse ( content = api_yaml , media_type = \"text/x-yaml\" )","title":"FairTest"},{"location":"FairTestAPI/","text":"FairTestAPI Bases: FastAPI Class to deploy a FAIR metrics tests API, it will create API calls for each FairTest defined in the metrics folder. main.py from fair_test import FairTestAPI app = FairTestAPI ( title = 'FAIR enough metrics tests API' , metrics_folder_path = 'metrics' , description = \"FAIR Metrics tests API for resources related to research. Follows the specifications described by the [FAIRMetrics](https://github.com/FAIRMetrics/Metrics) working group.\" , license_info = { \"name\" : \"MIT license\" , \"url\" : \"https://opensource.org/licenses/MIT\" }, ) Source code in fair_test/fair_test_api.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class FairTestAPI ( FastAPI ): \"\"\" Class to deploy a FAIR metrics tests API, it will create API calls for each FairTest defined in the `metrics` folder. ```python title=\"main.py\" from fair_test import FairTestAPI app = FairTestAPI( title='FAIR enough metrics tests API', metrics_folder_path='metrics', description=\"FAIR Metrics tests API for resources related to research. Follows the specifications described by the [FAIRMetrics](https://github.com/FAIRMetrics/Metrics) working group.\", license_info = { \"name\": \"MIT license\", \"url\": \"https://opensource.org/licenses/MIT\" }, ) ``` \"\"\" def __init__ ( self , * args , title : str = \"FAIR Metrics Test API\" , description = \"FAIR Metrics Test API for online resources. Follows the specifications described by the [FAIRMetrics](https://github.com/FAIRMetrics/Metrics) working group. \\n Built with the [**fair-test** library](https://maastrichtu-ids.github.io/fair-test)\" , version = \"0.1.0\" , cors_enabled = True , public_url = \"https://metrics.api.fair-enough.semanticscience.org\" , metrics_folder_path = \"metrics\" , contact = { \"name\" : settings . CONTACT_NAME , \"email\" : settings . CONTACT_EMAIL , \"url\" : settings . CONTACT_URL , \"x-id\" : settings . CONTACT_ORCID , }, license_info = { \"name\" : \"MIT license\" , \"url\" : \"https://opensource.org/licenses/MIT\" , }, ** kwargs , ) -> None : self . title = title self . description = description self . version = version self . public_url = public_url self . metrics_folder_path = metrics_folder_path # Instantiate FastAPI super () . __init__ ( title = title , description = description , version = version , contact = contact , license_info = license_info , ) if cors_enabled : self . add_middleware ( CORSMiddleware , allow_origins = [ \"*\" ], allow_credentials = True , allow_methods = [ \"*\" ], allow_headers = [ \"*\" ], ) metrics_module = metrics_folder_path . replace ( \"/\" , \".\" ) # First get the metrics tests filepath assess_name_list = self . get_metrics_tests_filepaths () # Then import each metric test listed in the metrics folder for assess_name in assess_name_list : assess_module = assess_name . replace ( \"/\" , \".\" ) import importlib MetricTest = getattr ( importlib . import_module ( f \" { metrics_module } . { assess_module } \" ), \"MetricTest\" , ) metric = MetricTest () try : # cf. https://github.com/tiangolo/fastapi/blob/master/fastapi/routing.py#L479 self . add_api_route ( path = f \"/tests/ { metric . metric_path } \" , methods = [ \"POST\" ], endpoint = metric . do_evaluate , name = metric . title , openapi_extra = { \"description\" : metric . description }, tags = [ metric . applies_to_principle ], ) self . add_api_route ( path = f \"/tests/ { metric . metric_path } \" , methods = [ \"GET\" ], endpoint = metric . openapi_yaml , name = metric . title , openapi_extra = { \"description\" : metric . description }, tags = [ metric . applies_to_principle ], ) except Exception : print ( \"\u274c No API defined for \" + metric . metric_path ) @self . get ( \"/\" , include_in_schema = False ) def redirect_root_to_docs (): # Redirect the route / to /docs return RedirectResponse ( url = \"/docs\" ) def get_metrics_tests_filepaths ( self ): assess_name_list = [] for path , subdirs , files in os . walk ( self . metrics_folder_path ): for filename in files : if not path . endswith ( \"__pycache__\" ) and not filename . endswith ( \"__init__.py\" ): filepath = path . replace ( self . metrics_folder_path , \"\" ) if filepath : assess_name_list . append ( filepath [ 1 :] + \"/\" + filename [: - 3 ]) else : assess_name_list . append ( filename [: - 3 ]) return assess_name_list def get_metrics_tests_tests ( self ): metrics_module = self . metrics_folder_path . replace ( \"/\" , \".\" ) metrics_paths = self . get_metrics_tests_filepaths () test_tests = [] for metrics_name in metrics_paths : assess_module = metrics_name . replace ( \"/\" , \".\" ) import importlib MetricTest = getattr ( importlib . import_module ( f \" { metrics_module } . { assess_module } \" ), \"MetricTest\" , ) metric = MetricTest () for subj , score in metric . test_test . items (): test_tests . append ({ \"subject\" : subj , \"score\" : score , \"metric_id\" : metric . metric_path }) return test_tests def run_tests ( self , test_endpoint , metric : str = None ): \"\"\" Run `pytest` tests for each metric test. URLs to test and expected scores are defined with the `test_test` attribute. Use this in a test file to automatically test all metrics tests, for example: ```python title='tests/test_metrics.py' from fastapi.testclient import TestClient from fair_test import FairTestAPI app = FairTestAPI(metrics_folder_path='metrics') endpoint = TestClient(app) def test_api(): app.run_tests(endpoint) ``` Parameters: test_endpoint (TestClient): FastAPI TestClient of the app to test \"\"\" eval_list = self . get_metrics_tests_tests () RED = \" \\033 [91m\" BOLD = \" \\033 [1m\" END = \" \\033 [0m\" YELLOW = \" \\033 [33m\" CYAN = \" \\033 [36m\" PURPLE = \" \\033 [95m\" BLUE = \" \\033 [34m\" GREEN = \" \\033 [32m\" if metric : run_evals = [ ev for ev in eval_list if ev [ \"metric_id\" ] == metric ] else : run_evals = eval_list print ( f \"\u23f3\ufe0f Running tests for { BOLD }{ len ( run_evals ) }{ END } metric/subject/score combinations\" ) # Test POST metrics evaluation request for eval in run_evals : if metric and metric != eval [ \"metric_id\" ]: continue print ( f \"Testing { YELLOW }{ eval [ 'subject' ] }{ END } with { CYAN }{ eval [ 'metric_id' ] }{ END } (expect { BOLD + str ( eval [ 'score' ]) + END } )\" ) r = test_endpoint . post ( f \"/tests/ { eval [ 'metric_id' ] } \" , json = { \"subject\" : eval [ \"subject\" ]}, headers = { \"Accept\" : \"application/json\" }, ) assert r . status_code == 200 res = r . json () # Check score: score = int ( res [ 0 ][ \"http://semanticscience.org/resource/SIO_000300\" ][ 0 ][ \"@value\" ]) if score != eval [ \"score\" ]: print ( f \"\u274c Wrong score: got { RED }{ score }{ END } instead of { RED }{ eval [ 'score' ] }{ END } for { BOLD }{ eval [ 'subject' ] }{ END } with the metric test { BOLD }{ eval [ 'metric_id' ] }{ END } \" ) assert score == eval [ \"score\" ] # Test get YAML metrics_id_to_test = set () for eval in eval_list : metrics_id_to_test . add ( eval [ \"metric_id\" ]) for metric_id in list ( metrics_id_to_test ): r = test_endpoint . get ( f \"/tests/ { metric_id } \" ) assert r . status_code == 200 api_yaml = yaml . load ( r . text , Loader = yaml . FullLoader ) assert api_yaml [ \"info\" ][ \"title\" ] assert api_yaml [ \"info\" ][ \"x-applies_to_principle\" ] assert api_yaml [ \"info\" ][ \"x-tests_metric\" ] # test bad request for metric_id in list ( metrics_id_to_test ): response = test_endpoint . post ( f \"/tests/ { metric_id } \" , json = { \"subject\" : \"\" }, headers = { \"accept\" : \"application/json\" }, ) assert response . status_code == 422 break # test 404 response = test_endpoint . get ( f \"/dont-exist\" , headers = { \"accept\" : \"application/json\" }) assert response . status_code == 404 # test redirect response = test_endpoint . get ( \"/\" ) assert response . status_code == 200 print ( f \"\u2705 Successfully tested a total of { BOLD }{ len ( run_evals ) }{ END } FAIR metrics tests/subjects/score combinations\" ) run_tests ( test_endpoint , metric = None ) Run pytest tests for each metric test. URLs to test and expected scores are defined with the test_test attribute. Use this in a test file to automatically test all metrics tests, for example: tests/test_metrics.py from fastapi.testclient import TestClient from fair_test import FairTestAPI app = FairTestAPI ( metrics_folder_path = 'metrics' ) endpoint = TestClient ( app ) def test_api (): app . run_tests ( endpoint ) Parameters: Name Type Description Default test_endpoint TestClient FastAPI TestClient of the app to test required Source code in fair_test/fair_test_api.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def run_tests ( self , test_endpoint , metric : str = None ): \"\"\" Run `pytest` tests for each metric test. URLs to test and expected scores are defined with the `test_test` attribute. Use this in a test file to automatically test all metrics tests, for example: ```python title='tests/test_metrics.py' from fastapi.testclient import TestClient from fair_test import FairTestAPI app = FairTestAPI(metrics_folder_path='metrics') endpoint = TestClient(app) def test_api(): app.run_tests(endpoint) ``` Parameters: test_endpoint (TestClient): FastAPI TestClient of the app to test \"\"\" eval_list = self . get_metrics_tests_tests () RED = \" \\033 [91m\" BOLD = \" \\033 [1m\" END = \" \\033 [0m\" YELLOW = \" \\033 [33m\" CYAN = \" \\033 [36m\" PURPLE = \" \\033 [95m\" BLUE = \" \\033 [34m\" GREEN = \" \\033 [32m\" if metric : run_evals = [ ev for ev in eval_list if ev [ \"metric_id\" ] == metric ] else : run_evals = eval_list print ( f \"\u23f3\ufe0f Running tests for { BOLD }{ len ( run_evals ) }{ END } metric/subject/score combinations\" ) # Test POST metrics evaluation request for eval in run_evals : if metric and metric != eval [ \"metric_id\" ]: continue print ( f \"Testing { YELLOW }{ eval [ 'subject' ] }{ END } with { CYAN }{ eval [ 'metric_id' ] }{ END } (expect { BOLD + str ( eval [ 'score' ]) + END } )\" ) r = test_endpoint . post ( f \"/tests/ { eval [ 'metric_id' ] } \" , json = { \"subject\" : eval [ \"subject\" ]}, headers = { \"Accept\" : \"application/json\" }, ) assert r . status_code == 200 res = r . json () # Check score: score = int ( res [ 0 ][ \"http://semanticscience.org/resource/SIO_000300\" ][ 0 ][ \"@value\" ]) if score != eval [ \"score\" ]: print ( f \"\u274c Wrong score: got { RED }{ score }{ END } instead of { RED }{ eval [ 'score' ] }{ END } for { BOLD }{ eval [ 'subject' ] }{ END } with the metric test { BOLD }{ eval [ 'metric_id' ] }{ END } \" ) assert score == eval [ \"score\" ] # Test get YAML metrics_id_to_test = set () for eval in eval_list : metrics_id_to_test . add ( eval [ \"metric_id\" ]) for metric_id in list ( metrics_id_to_test ): r = test_endpoint . get ( f \"/tests/ { metric_id } \" ) assert r . status_code == 200 api_yaml = yaml . load ( r . text , Loader = yaml . FullLoader ) assert api_yaml [ \"info\" ][ \"title\" ] assert api_yaml [ \"info\" ][ \"x-applies_to_principle\" ] assert api_yaml [ \"info\" ][ \"x-tests_metric\" ] # test bad request for metric_id in list ( metrics_id_to_test ): response = test_endpoint . post ( f \"/tests/ { metric_id } \" , json = { \"subject\" : \"\" }, headers = { \"accept\" : \"application/json\" }, ) assert response . status_code == 422 break # test 404 response = test_endpoint . get ( f \"/dont-exist\" , headers = { \"accept\" : \"application/json\" }) assert response . status_code == 404 # test redirect response = test_endpoint . get ( \"/\" ) assert response . status_code == 200 print ( f \"\u2705 Successfully tested a total of { BOLD }{ len ( run_evals ) }{ END } FAIR metrics tests/subjects/score combinations\" )","title":"<span><i class='fa-solid fa-flask-vial'></i>&nbsp;&nbsp;FairTestAPI</span>"},{"location":"FairTestAPI/#fairtestapi","text":"Bases: FastAPI Class to deploy a FAIR metrics tests API, it will create API calls for each FairTest defined in the metrics folder. main.py from fair_test import FairTestAPI app = FairTestAPI ( title = 'FAIR enough metrics tests API' , metrics_folder_path = 'metrics' , description = \"FAIR Metrics tests API for resources related to research. Follows the specifications described by the [FAIRMetrics](https://github.com/FAIRMetrics/Metrics) working group.\" , license_info = { \"name\" : \"MIT license\" , \"url\" : \"https://opensource.org/licenses/MIT\" }, ) Source code in fair_test/fair_test_api.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class FairTestAPI ( FastAPI ): \"\"\" Class to deploy a FAIR metrics tests API, it will create API calls for each FairTest defined in the `metrics` folder. ```python title=\"main.py\" from fair_test import FairTestAPI app = FairTestAPI( title='FAIR enough metrics tests API', metrics_folder_path='metrics', description=\"FAIR Metrics tests API for resources related to research. Follows the specifications described by the [FAIRMetrics](https://github.com/FAIRMetrics/Metrics) working group.\", license_info = { \"name\": \"MIT license\", \"url\": \"https://opensource.org/licenses/MIT\" }, ) ``` \"\"\" def __init__ ( self , * args , title : str = \"FAIR Metrics Test API\" , description = \"FAIR Metrics Test API for online resources. Follows the specifications described by the [FAIRMetrics](https://github.com/FAIRMetrics/Metrics) working group. \\n Built with the [**fair-test** library](https://maastrichtu-ids.github.io/fair-test)\" , version = \"0.1.0\" , cors_enabled = True , public_url = \"https://metrics.api.fair-enough.semanticscience.org\" , metrics_folder_path = \"metrics\" , contact = { \"name\" : settings . CONTACT_NAME , \"email\" : settings . CONTACT_EMAIL , \"url\" : settings . CONTACT_URL , \"x-id\" : settings . CONTACT_ORCID , }, license_info = { \"name\" : \"MIT license\" , \"url\" : \"https://opensource.org/licenses/MIT\" , }, ** kwargs , ) -> None : self . title = title self . description = description self . version = version self . public_url = public_url self . metrics_folder_path = metrics_folder_path # Instantiate FastAPI super () . __init__ ( title = title , description = description , version = version , contact = contact , license_info = license_info , ) if cors_enabled : self . add_middleware ( CORSMiddleware , allow_origins = [ \"*\" ], allow_credentials = True , allow_methods = [ \"*\" ], allow_headers = [ \"*\" ], ) metrics_module = metrics_folder_path . replace ( \"/\" , \".\" ) # First get the metrics tests filepath assess_name_list = self . get_metrics_tests_filepaths () # Then import each metric test listed in the metrics folder for assess_name in assess_name_list : assess_module = assess_name . replace ( \"/\" , \".\" ) import importlib MetricTest = getattr ( importlib . import_module ( f \" { metrics_module } . { assess_module } \" ), \"MetricTest\" , ) metric = MetricTest () try : # cf. https://github.com/tiangolo/fastapi/blob/master/fastapi/routing.py#L479 self . add_api_route ( path = f \"/tests/ { metric . metric_path } \" , methods = [ \"POST\" ], endpoint = metric . do_evaluate , name = metric . title , openapi_extra = { \"description\" : metric . description }, tags = [ metric . applies_to_principle ], ) self . add_api_route ( path = f \"/tests/ { metric . metric_path } \" , methods = [ \"GET\" ], endpoint = metric . openapi_yaml , name = metric . title , openapi_extra = { \"description\" : metric . description }, tags = [ metric . applies_to_principle ], ) except Exception : print ( \"\u274c No API defined for \" + metric . metric_path ) @self . get ( \"/\" , include_in_schema = False ) def redirect_root_to_docs (): # Redirect the route / to /docs return RedirectResponse ( url = \"/docs\" ) def get_metrics_tests_filepaths ( self ): assess_name_list = [] for path , subdirs , files in os . walk ( self . metrics_folder_path ): for filename in files : if not path . endswith ( \"__pycache__\" ) and not filename . endswith ( \"__init__.py\" ): filepath = path . replace ( self . metrics_folder_path , \"\" ) if filepath : assess_name_list . append ( filepath [ 1 :] + \"/\" + filename [: - 3 ]) else : assess_name_list . append ( filename [: - 3 ]) return assess_name_list def get_metrics_tests_tests ( self ): metrics_module = self . metrics_folder_path . replace ( \"/\" , \".\" ) metrics_paths = self . get_metrics_tests_filepaths () test_tests = [] for metrics_name in metrics_paths : assess_module = metrics_name . replace ( \"/\" , \".\" ) import importlib MetricTest = getattr ( importlib . import_module ( f \" { metrics_module } . { assess_module } \" ), \"MetricTest\" , ) metric = MetricTest () for subj , score in metric . test_test . items (): test_tests . append ({ \"subject\" : subj , \"score\" : score , \"metric_id\" : metric . metric_path }) return test_tests def run_tests ( self , test_endpoint , metric : str = None ): \"\"\" Run `pytest` tests for each metric test. URLs to test and expected scores are defined with the `test_test` attribute. Use this in a test file to automatically test all metrics tests, for example: ```python title='tests/test_metrics.py' from fastapi.testclient import TestClient from fair_test import FairTestAPI app = FairTestAPI(metrics_folder_path='metrics') endpoint = TestClient(app) def test_api(): app.run_tests(endpoint) ``` Parameters: test_endpoint (TestClient): FastAPI TestClient of the app to test \"\"\" eval_list = self . get_metrics_tests_tests () RED = \" \\033 [91m\" BOLD = \" \\033 [1m\" END = \" \\033 [0m\" YELLOW = \" \\033 [33m\" CYAN = \" \\033 [36m\" PURPLE = \" \\033 [95m\" BLUE = \" \\033 [34m\" GREEN = \" \\033 [32m\" if metric : run_evals = [ ev for ev in eval_list if ev [ \"metric_id\" ] == metric ] else : run_evals = eval_list print ( f \"\u23f3\ufe0f Running tests for { BOLD }{ len ( run_evals ) }{ END } metric/subject/score combinations\" ) # Test POST metrics evaluation request for eval in run_evals : if metric and metric != eval [ \"metric_id\" ]: continue print ( f \"Testing { YELLOW }{ eval [ 'subject' ] }{ END } with { CYAN }{ eval [ 'metric_id' ] }{ END } (expect { BOLD + str ( eval [ 'score' ]) + END } )\" ) r = test_endpoint . post ( f \"/tests/ { eval [ 'metric_id' ] } \" , json = { \"subject\" : eval [ \"subject\" ]}, headers = { \"Accept\" : \"application/json\" }, ) assert r . status_code == 200 res = r . json () # Check score: score = int ( res [ 0 ][ \"http://semanticscience.org/resource/SIO_000300\" ][ 0 ][ \"@value\" ]) if score != eval [ \"score\" ]: print ( f \"\u274c Wrong score: got { RED }{ score }{ END } instead of { RED }{ eval [ 'score' ] }{ END } for { BOLD }{ eval [ 'subject' ] }{ END } with the metric test { BOLD }{ eval [ 'metric_id' ] }{ END } \" ) assert score == eval [ \"score\" ] # Test get YAML metrics_id_to_test = set () for eval in eval_list : metrics_id_to_test . add ( eval [ \"metric_id\" ]) for metric_id in list ( metrics_id_to_test ): r = test_endpoint . get ( f \"/tests/ { metric_id } \" ) assert r . status_code == 200 api_yaml = yaml . load ( r . text , Loader = yaml . FullLoader ) assert api_yaml [ \"info\" ][ \"title\" ] assert api_yaml [ \"info\" ][ \"x-applies_to_principle\" ] assert api_yaml [ \"info\" ][ \"x-tests_metric\" ] # test bad request for metric_id in list ( metrics_id_to_test ): response = test_endpoint . post ( f \"/tests/ { metric_id } \" , json = { \"subject\" : \"\" }, headers = { \"accept\" : \"application/json\" }, ) assert response . status_code == 422 break # test 404 response = test_endpoint . get ( f \"/dont-exist\" , headers = { \"accept\" : \"application/json\" }) assert response . status_code == 404 # test redirect response = test_endpoint . get ( \"/\" ) assert response . status_code == 200 print ( f \"\u2705 Successfully tested a total of { BOLD }{ len ( run_evals ) }{ END } FAIR metrics tests/subjects/score combinations\" )","title":"FairTestAPI"},{"location":"FairTestAPI/#fair_test.fair_test_api.FairTestAPI.run_tests","text":"Run pytest tests for each metric test. URLs to test and expected scores are defined with the test_test attribute. Use this in a test file to automatically test all metrics tests, for example: tests/test_metrics.py from fastapi.testclient import TestClient from fair_test import FairTestAPI app = FairTestAPI ( metrics_folder_path = 'metrics' ) endpoint = TestClient ( app ) def test_api (): app . run_tests ( endpoint ) Parameters: Name Type Description Default test_endpoint TestClient FastAPI TestClient of the app to test required Source code in fair_test/fair_test_api.py 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def run_tests ( self , test_endpoint , metric : str = None ): \"\"\" Run `pytest` tests for each metric test. URLs to test and expected scores are defined with the `test_test` attribute. Use this in a test file to automatically test all metrics tests, for example: ```python title='tests/test_metrics.py' from fastapi.testclient import TestClient from fair_test import FairTestAPI app = FairTestAPI(metrics_folder_path='metrics') endpoint = TestClient(app) def test_api(): app.run_tests(endpoint) ``` Parameters: test_endpoint (TestClient): FastAPI TestClient of the app to test \"\"\" eval_list = self . get_metrics_tests_tests () RED = \" \\033 [91m\" BOLD = \" \\033 [1m\" END = \" \\033 [0m\" YELLOW = \" \\033 [33m\" CYAN = \" \\033 [36m\" PURPLE = \" \\033 [95m\" BLUE = \" \\033 [34m\" GREEN = \" \\033 [32m\" if metric : run_evals = [ ev for ev in eval_list if ev [ \"metric_id\" ] == metric ] else : run_evals = eval_list print ( f \"\u23f3\ufe0f Running tests for { BOLD }{ len ( run_evals ) }{ END } metric/subject/score combinations\" ) # Test POST metrics evaluation request for eval in run_evals : if metric and metric != eval [ \"metric_id\" ]: continue print ( f \"Testing { YELLOW }{ eval [ 'subject' ] }{ END } with { CYAN }{ eval [ 'metric_id' ] }{ END } (expect { BOLD + str ( eval [ 'score' ]) + END } )\" ) r = test_endpoint . post ( f \"/tests/ { eval [ 'metric_id' ] } \" , json = { \"subject\" : eval [ \"subject\" ]}, headers = { \"Accept\" : \"application/json\" }, ) assert r . status_code == 200 res = r . json () # Check score: score = int ( res [ 0 ][ \"http://semanticscience.org/resource/SIO_000300\" ][ 0 ][ \"@value\" ]) if score != eval [ \"score\" ]: print ( f \"\u274c Wrong score: got { RED }{ score }{ END } instead of { RED }{ eval [ 'score' ] }{ END } for { BOLD }{ eval [ 'subject' ] }{ END } with the metric test { BOLD }{ eval [ 'metric_id' ] }{ END } \" ) assert score == eval [ \"score\" ] # Test get YAML metrics_id_to_test = set () for eval in eval_list : metrics_id_to_test . add ( eval [ \"metric_id\" ]) for metric_id in list ( metrics_id_to_test ): r = test_endpoint . get ( f \"/tests/ { metric_id } \" ) assert r . status_code == 200 api_yaml = yaml . load ( r . text , Loader = yaml . FullLoader ) assert api_yaml [ \"info\" ][ \"title\" ] assert api_yaml [ \"info\" ][ \"x-applies_to_principle\" ] assert api_yaml [ \"info\" ][ \"x-tests_metric\" ] # test bad request for metric_id in list ( metrics_id_to_test ): response = test_endpoint . post ( f \"/tests/ { metric_id } \" , json = { \"subject\" : \"\" }, headers = { \"accept\" : \"application/json\" }, ) assert response . status_code == 422 break # test 404 response = test_endpoint . get ( f \"/dont-exist\" , headers = { \"accept\" : \"application/json\" }) assert response . status_code == 404 # test redirect response = test_endpoint . get ( \"/\" ) assert response . status_code == 200 print ( f \"\u2705 Successfully tested a total of { BOLD }{ len ( run_evals ) }{ END } FAIR metrics tests/subjects/score combinations\" )","title":"run_tests()"},{"location":"FairTestEvaluation/","text":"FairTestEvaluation Bases: BaseModel Class to manipulate a FAIR metrics test evaluation. Provides helpers functions to easily retrieve and parse metadata. A new FairTestEvaluation object is create for each new request to one of the FAIR metrics test API call exposed by the API metrics/a1_check_something.py from fair_test import FairTest , FairTestEvaluation class MetricTest ( FairTest ): metric_path = 'a1-check-something' applies_to_principle = 'A1' title = 'Check something' description = \"Test something\" author = 'https://orcid.org/0000-0000-0000-0000' metric_version = '0.1.0' test_test = { 'http://doi.org/10.1594/PANGAEA.908011' : 1 , 'https://github.com/MaastrichtU-IDS/fair-test' : 0 , } def evaluate ( self , eval : FairTestEvaluation ): eval . info ( f 'Checking something for { self . subject } ' ) g = eval . retrieve_metadata ( self . subject , use_harvester = False ) if len ( g ) > 0 : eval . success ( f ' { len ( g ) } triples found, test sucessful' ) else : eval . failure ( 'No triples found, test failed' ) return eval . response () Source code in fair_test/fair_test_evaluation.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 class FairTestEvaluation ( BaseModel ): \"\"\" Class to manipulate a FAIR metrics test evaluation. Provides helpers functions to easily retrieve and parse metadata. A new FairTestEvaluation object is create for each new request to one of the FAIR metrics test API call exposed by the API ```python title=\"metrics/a1_check_something.py\" from fair_test import FairTest, FairTestEvaluation class MetricTest(FairTest): metric_path = 'a1-check-something' applies_to_principle = 'A1' title = 'Check something' description = \"Test something\" author = 'https://orcid.org/0000-0000-0000-0000' metric_version = '0.1.0' test_test={ 'http://doi.org/10.1594/PANGAEA.908011': 1, 'https://github.com/MaastrichtU-IDS/fair-test': 0, } def evaluate(self, eval: FairTestEvaluation): eval.info(f'Checking something for {self.subject}') g = eval.retrieve_metadata(self.subject, use_harvester=False) if len(g) > 0: eval.success(f'{len(g)} triples found, test sucessful') else: eval.failure('No triples found, test failed') return eval.response() ``` \"\"\" subject : Optional [ str ] subject_url : Optional [ str ] score : int = 0 score_bonus : int = 0 comment : List = [] date : str = datetime . datetime . now () . strftime ( \"%Y-%m- %d T%H:%M:%S+01:00\" ) metric_version : str = \"0.1.0\" id : Optional [ str ] # URL of the test results data : Optional [ dict ] = {} def __init__ ( self , subject : str , metric_path : str ) -> None : super () . __init__ () self . subject = subject self . id = f \" { settings . HOST_URL } /metrics/ { metric_path } # { quote ( str ( self . subject )) } /result- { self . date } \" self . subject_url = self . get_url ( subject ) # Add potential alternative URIs for the subject if self . subject_url : alt_uris = set () alt_uris . add ( self . subject_url ) # Add HTTPS/HTTPS counterpart as alternative URIs if self . subject_url . startswith ( \"http://\" ): alt_uris . add ( self . subject_url . replace ( \"http://\" , \"https://\" )) elif self . subject . startswith ( \"https://\" ): alt_uris . add ( self . subject_url . replace ( \"https://\" , \"http://\" )) # Fix to add an alternative URI for doi.org that is commonly used as identifier in the metadata parsed_url = urlparse ( self . subject_url ) if parsed_url . netloc and parsed_url . netloc == \"doi.org\" : alt_uris . add ( \"http://dx.doi.org/\" + parsed_url . path [ 1 :]) self . data [ \"alternative_uris\" ] = list ( alt_uris ) class Config : arbitrary_types_allowed = True def get_url ( self , id : str ) -> str : \"\"\"Return the full URL for a given identifiers (e.g. URL, DOI, handle)\"\"\" if idutils . is_url ( id ): self . info ( f \"Validated the resource { id } is a URL\" ) return id if idutils . is_doi ( id ): self . info ( f \"Validated the resource { id } is a DOI\" ) return idutils . to_url ( id , \"doi\" , \"https\" ) if idutils . is_handle ( id ): self . info ( f \"Validated the resource { id } is a handle\" ) return idutils . to_url ( id , \"handle\" , \"https\" ) # TODO: add INCHI key? # inchikey = regexp(/^\\w{14}\\-\\w{10}\\-\\w$/) # return f\"https://pubchem.ncbi.nlm.nih.gov/rest/rdf/inchikey/{inchikey}\" self . warn ( f \"Could not validate the given resource URI { id } is a URL, DOI, or handle\" ) return None # TODO: implement metadata extraction with more tools? # e.g. Apache Tika for PDF/pptx? or ruby Kellog's Distiller? http://rdf.greggkellogg.net/distiller # c.f. https://github.com/FAIRMetrics/Metrics/blob/master/MetricsEvaluatorCode/Ruby/metrictests/fair_metrics_utilities.rb def retrieve_metadata ( self , url : str , use_harvester : Optional [ bool ] = False , harvester_url : Optional [ str ] = \"https://fair-tests.137.120.31.101.nip.io/tests/harvester\" , ) -> Any : \"\"\" Retrieve metadata from a URL, RDF metadata parsed as a RDFLib Graph in priority. Super useful. It tries: - Following signposting links (returned in HTTP headers) - Extracting JSON-LD embedded in the HTML - Asking RDF through content-negociation - Can return JSON found as a fallback, if RDF metadata is not found You can also use an external harvester API to get the RDF metadata Parameters: url: URL to retrieve RDF from use_harvester: Use an external harvester to retrieve the RDF instead of the built-in python harvester harvester_url: URL of the RDF harvester used Returns: g (Graph): A RDFLib Graph with the RDF found at the given URL \"\"\" original_url = url url = self . get_url ( url ) if not url : self . warn ( f \"The resource { original_url } could not be converted to a valid URL, hence no metadata could be retrieved\" ) return [] if use_harvester == True : # Check the harvester response: # curl -X POST -d '{\"subject\": \"https://doi.org/10.1594/PANGAEA.908011\"}' https://fair-tests.137.120.31.101.nip.io/tests/harvester try : self . info ( f \"Using Harvester at { harvester_url } to retrieve RDF metadata at { url } \" ) res = requests . post ( harvester_url , json = { \"subject\" : url }, timeout = 60 , # headers={\"Accept\": \"application/ld+json\"} ) return self . parse_rdf ( res . text , \"text/turtle\" , log_msg = \"FAIR evaluator harvester RDF\" ) except Exception : self . warn ( f \"Failed to reach the Harvester at { harvester_url } , using the built-in python harvester\" ) # https://github.com/FAIRMetrics/Metrics/blob/master/MetricsEvaluatorCode/Ruby/metrictests/fair_metrics_utilities.rb#L355 html_text = None metadata_obj = [] # Check if URL resolve and if redirection # r = requests.head(url) try : r = requests . get ( url ) r . raise_for_status () # Raises a HTTPError if the status is 4xx, 5xxx self . info ( f \"Successfully resolved { url } \" ) html_text = r . text if r . history : # Extract alternative URIs if request redirected redirect_url = r . url if redirect_url . startswith ( \"https://linkinghub.elsevier.com/retrieve/pii/\" ): # Special case to handle Elsevier bad redirections to ScienceDirect redirect_url = redirect_url . replace ( \"https://linkinghub.elsevier.com/retrieve/pii/\" , \"https://www.sciencedirect.com/science/article/pii/\" , ) self . data [ \"redirect_url\" ] = redirect_url if url == self . subject and not redirect_url in self . data [ \"alternative_uris\" ]: self . info ( f \"Request was redirected to { redirect_url } , adding to the list of alternative URIs for the subject\" ) self . data [ \"alternative_uris\" ] . append ( redirect_url ) if r . url . startswith ( \"http://\" ): self . data [ \"alternative_uris\" ] . append ( redirect_url . replace ( \"http://\" , \"https://\" )) elif r . url . startswith ( \"https://\" ): self . data [ \"alternative_uris\" ] . append ( redirect_url . replace ( \"https://\" , \"http://\" )) # Handle signposting links headers https://signposting.org/FAIR if r . links : # Follow signposting links, this could create a lot of recursions (to be checked) self . info ( f \"Found Signposting links: { str ( r . links ) } \" ) self . data [ \"signposting_links\" ] = r . links check_rels = [ \"alternate\" , \"describedby\" , \"meta\" ] # Alternate is used by schema.org for rel in check_rels : if rel in r . links . keys (): rel_url = r . links [ rel ][ \"url\" ] if not rel_url . startswith ( \"http://\" ) and not rel_url . startswith ( \"https://\" ): # In some case the rel URL provided is relative to the requested URL if r . url . endswith ( \"/\" ) and rel_url . startswith ( \"/\" ): rel_url = rel_url [ 1 :] rel_url = r . url + rel_url metadata_obj = self . retrieve_metadata ( rel_url ) if len ( metadata_obj ) > 0 : return metadata_obj except Exception as e : self . warn ( f \"Error resolving the URL { url } : { str ( e . args [ 0 ]) } \" ) self . info ( \"Checking for metadata embedded in the HTML page returned by the resource URI \" + url + \" using extruct\" ) # TODO: support client-side JS generated HTML using Selenium https://github.com/vemonet/extruct-selenium try : extructed = extruct . extract ( html_text . encode ( \"utf8\" )) if url == self . subject : self . data [ \"extruct\" ] = extructed if len ( extructed [ \"json-ld\" ]) > 0 : g = self . parse_rdf ( extructed [ \"json-ld\" ], \"json-ld\" , log_msg = \"HTML embedded JSON-LD RDF\" ) if len ( g ) > 0 : self . info ( f \"Found JSON-LD RDF metadata embedded in the HTML with extruct\" ) return g else : metadata_obj = extructed [ \"json-ld\" ] if len ( extructed [ \"rdfa\" ]) > 0 : g = self . parse_rdf ( extructed [ \"rdfa\" ], \"json-ld\" , log_msg = \"HTML embedded RDFa\" ) if len ( g ) > 0 : self . info ( f \"Found RDFa metadata embedded in the HTML with extruct\" ) return g elif not metadata_obj : metadata_obj = extructed [ \"rdfa\" ] if not metadata_obj and len ( extructed [ \"microdata\" ]) > 0 : metadata_obj = extructed [ \"microdata\" ] if not metadata_obj and extructed [ \"dublincore\" ] != [{ \"namespaces\" : {}, \"elements\" : [], \"terms\" : []}]: # Dublin core always comes as this empty dict if no match metadata_obj = extructed [ \"dublincore\" ] # The rest is not extracted because usually give no interesting metadata: # opengraph, microformat except Exception as e : self . info ( f \"Error when running extruct on { url } . Getting: { str ( e . args [ 0 ]) } \" ) # Perform content negociation last because it's the slowest for a lot of URLs like zenodo # We need to do direct content negociation to turtle and json # because some URLs dont support standard weighted content negociation check_mime_types = [ \"text/turtle\" , \"application/ld+json\" , \"text/turtle, application/turtle, application/x-turtle;q=0.9, application/ld+json;q=0.8, application/rdf+xml, text/n3, text/rdf+n3;q=0.7\" , ] for mime_type in check_mime_types : try : r = requests . get ( url , headers = { \"accept\" : mime_type }) r . raise_for_status () # Raises a HTTPError if the status is 4xx, 5xxx content_type = r . headers [ \"Content-Type\" ] . replace ( \" \" , \"\" ) . replace ( \";charset=utf-8\" , \"\" ) # If return text/plain we parse as turtle or JSON-LD # content_type = content_type.replace('text/plain', 'text/turtle') self . info ( f \"Content-negotiation: found some metadata in { content_type } when asking for { mime_type } \" ) try : # If returns JSON self . data [ \"json-ld\" ] = r . json () if not metadata_obj : metadata_obj = r . json () return self . parse_rdf ( r . json (), \"json-ld\" , log_msg = \"content negotiation JSON-LD RDF\" ) except Exception : # If returns RDF as text, such as turtle return self . parse_rdf ( r . text , content_type , log_msg = \"content negotiation RDF\" ) except Exception as e : self . info ( f \"Content-negotiation: error with { url } when asking for { mime_type } . Getting { str ( e . args [ 0 ]) } \" ) # Error: e.args[0] return metadata_obj def parse_rdf ( self , rdf_data : Any , mime_type : Optional [ str ] = None , log_msg : Optional [ str ] = \"\" , ) -> Any : \"\"\" Parse any string or JSON-like object to a RDFLib Graph Parameters: rdf_data (str|object): Text or object to convert to RDF mime_type: Mime type of the data to convert log_msg: Text to use when logging about the parsing process (help debugging) Returns: g (Graph): A RDFLib Graph \"\"\" # https://rdflib.readthedocs.io/en/stable/plugin_parsers.html parse_formats = [ \"turtle\" , \"json-ld\" , \"xml\" , \"ntriples\" , \"nquads\" , \"trig\" , \"n3\" ] if type ( rdf_data ) == dict : rdf_data = [ rdf_data ] if type ( rdf_data ) == list : for rdf_entry in rdf_data : try : # Dirty hack to fix RDFLib that is not able to parse JSON-LD schema.org (https://github.com/schemaorg/schemaorg/issues/2578) if \"@context\" in rdf_entry : if isinstance ( rdf_entry [ \"@context\" ], str ): if rdf_entry [ \"@context\" ] . startswith ( \"http://schema.org\" ) or rdf_entry [ \"@context\" ] . startswith ( \"https://schema.org\" ): rdf_entry [ \"@context\" ] = \"https://schema.org/docs/jsonldcontext.json\" if isinstance ( rdf_entry [ \"@context\" ], list ): for i , cont in enumerate ( rdf_entry [ \"@context\" ]): if isinstance ( cont , str ): rdf_entry [ \"@context\" ][ i ] = \"https://schema.org/docs/jsonldcontext.json\" except : pass # RDFLib JSON-LD had issue with encoding: https://github.com/RDFLib/rdflib/issues/1416 rdf_data = jsonld . expand ( rdf_data ) rdf_data = json . dumps ( rdf_data ) parse_formats = [ \"json-ld\" ] else : # Try to guess the format to parse from mime type mime_type = mime_type . split ( \";\" )[ 0 ] if \"turtle\" in mime_type : parse_formats = [ \"turtle\" ] elif \"xml\" in mime_type : parse_formats = [ \"xml\" ] elif \"ntriples\" in mime_type : parse_formats = [ \"ntriples\" ] elif \"nquads\" in mime_type : parse_formats = [ \"nquads\" ] elif \"trig\" in mime_type : parse_formats = [ \"trig\" ] # elif mime_type.startswith('text/html'): # parse_formats = [] g = ConjunctiveGraph () # Remove some auto-generated triples about the HTML content remove_preds = [ \"http://www.w3.org/1999/xhtml/vocab#role\" ] for rdf_format in parse_formats : try : g = ConjunctiveGraph () g . parse ( data = rdf_data , format = rdf_format ) for rm_pred in remove_preds : g . remove (( None , URIRef ( rm_pred ), None )) self . info ( f \"Successfully parsed { mime_type } RDF from { log_msg } with parser { rdf_format } , containing { str ( len ( g )) } triples\" ) return g except Exception as e : self . info ( f \"Could not parse { mime_type } metadata from { log_msg } with parser { rdf_format } . Getting error: { str ( e ) } \" ) return g # return None def extract_prop ( self , g : Any , preds : List [ Any ], subj : Optional [ Any ] = None ) -> List [ Any ]: \"\"\" Helper to extract properties from a RDFLib Graph Parameters: g (Graph): RDFLib Graph preds: List of predicates to find value for subj: Optionally also limit the results for a list of subjects Returns: props: A list of the values found for the given properties \"\"\" values = set () check_preds = set () for pred in preds : # Add the http/https counterpart for each predicate check_preds . add ( URIRef ( str ( pred ))) if str ( pred ) . startswith ( \"http://\" ): check_preds . add ( URIRef ( str ( pred ) . replace ( \"http://\" , \"https://\" ))) elif str ( pred ) . startswith ( \"https://\" ): check_preds . add ( URIRef ( str ( pred ) . replace ( \"https://\" , \"http://\" ))) # self.info(f\"Checking properties values for properties: {preds}\") # if subj: # self.info(f\"Checking properties values for subject URI(s): {str(subj)}\") for pred in list ( check_preds ): if not isinstance ( subj , list ): subj = [ subj ] # test_subjs = [URIRef(str(s)) for s in subj] for test_subj in subj : for s , p , o in g . triples (( test_subj , URIRef ( str ( pred )), None )): self . info ( f \"Found a value for a property { str ( pred ) } => { str ( o ) } \" ) values . add ( o ) return list ( values ) def extract_metadata_subject ( self , g : Any , alt_uris : Optional [ List [ str ]] = None ) -> Any : \"\"\" Helper to extract the subject URI to which metadata about the resource is attached in a RDFLib Graph Parameters: g (Graph): RDFLib Graph alt_uris: List of alternative URIs for the subject to find Returns: subject_uri: The subject URI used as ID in the metadata \"\"\" subject_uri = None if not alt_uris : alt_uris = self . data [ \"alternative_uris\" ] preds_id = [ \"https://purl.org/dc/terms/identifier\" , \"https://purl.org/dc/elements/1.1/identifier\" , \"https://schema.org/identifier\" , \"https://schema.org/sameAs\" , \"http://ogp.me/ns#url\" , ] all_preds_id = [ p . replace ( \"https://\" , \"http://\" ) for p in preds_id ] + preds_id all_preds_uris = [ URIRef ( str ( s )) for s in all_preds_id ] resource_properties = {} resource_linked_to = {} for alt_uri in alt_uris : uri_ref = URIRef ( str ( alt_uri )) # Search with the subject URI as triple subject for s , p , o in g . triples (( uri_ref , None , None )): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_properties [ str ( p )] = str ( o ) subject_uri = uri_ref if not subject_uri : # Search with the subject URI as triple object for pred in all_preds_uris : for s , p , o in g . triples (( None , pred , uri_ref )): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_linked_to [ str ( s )] = str ( p ) subject_uri = s if not subject_uri : # Also check when the subject URI defined as Literal for s , p , o in g . triples (( None , pred , Literal ( str ( uri_ref )))): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_linked_to [ str ( s )] = str ( p ) subject_uri = s if len ( resource_properties . keys ()) > 0 or len ( resource_linked_to . keys ()) > 0 : if not \"identifier_in_metadata\" in self . data . keys (): self . data [ \"identifier_in_metadata\" ] = {} if len ( resource_properties . keys ()) > 0 : self . data [ \"identifier_in_metadata\" ][ \"properties\" ] = resource_properties if len ( resource_linked_to . keys ()) > 0 : self . data [ \"identifier_in_metadata\" ][ \"linked_to\" ] = resource_linked_to return subject_uri def extract_data_subject ( self , g : Any , subject_uri : Optional [ List [ Any ]] = None ) -> List [ Any ]: \"\"\" Helper to easily retrieve the subject URI of the data from RDF metadata (RDFLib Graph) Parameters: g (Graph): RDFLib Graph subject_uri: metadata subject URI Returns: data_uri (list): List of URI found for the data in the metadata \"\"\" data_props = [ \"https://www.w3.org/ns/ldp#contains\" , \"https://xmlns.com/foaf/0.1/primaryTopic\" , \"https://schema.org/about\" , \"https://schema.org/mainEntity\" , \"https://schema.org/codeRepository\" , \"https://schema.org/distribution\" , \"https://www.w3.org/ns/dcat#distribution\" , \"https://semanticscience.org/resource/SIO_000332\" , \"https://semanticscience.org/resource/is-about\" , \"https://purl.obolibrary.org/obo/IAO_0000136\" , ] # http_props = [p.replace('https://', 'http://') for p in data_props] if not subject_uri : subject_uri = [ URIRef ( str ( s )) for s in self . data [ \"alternative_uris\" ]] self . info ( f \"Searching for the data URI using the following predicates: { ', ' . join ( data_props ) } \" ) data_uris = self . extract_prop ( g , preds = data_props , subj = subject_uri ) # Also extract data download URL when possible content_props = [ \"https://schema.org/url\" , \"https://schema.org/contentUrl\" , \"http://www.w3.org/ns/dcat#downloadURL\" , ] self . info ( f \"Checking if the data URI point to a download URL using one of the following predicates: { ', ' . join ( content_props ) } \" ) extracted_urls = set () for data_uri in data_uris : if isinstance ( data_uri , BNode ): content_urls = self . extract_prop ( g , preds = content_props , subj = data_uri ) for content_url in content_urls : extracted_urls . add ( str ( content_url )) else : extracted_urls . add ( str ( data_uri )) if not \"data_url\" in self . data . keys (): self . data [ \"content_url\" ] = [] self . data [ \"content_url\" ] = self . data [ \"content_url\" ] + list ( extracted_urls ) return data_uris def response ( self ) -> JSONResponse : \"\"\" Function used to generate the FAIR metric test results as JSON-LD, and return this JSON-LD as HTTP response Returns: response: HTTP response containing the test results as JSON-LD \"\"\" return JSONResponse ( self . to_jsonld ()) def to_jsonld ( self ) -> List [ Dict ]: # To see the object used by the original FAIR metrics: # curl -L -X 'POST' -d '{\"subject\": \"\"}' 'https://w3id.org/FAIR_Tests/tests/gen2_unique_identifier' return [ { \"@id\" : self . id , \"@type\" : [ \"http://fairmetrics.org/resources/metric_evaluation_result\" ], \"http://purl.obolibrary.org/obo/date\" : [ { \"@value\" : self . date , \"@type\" : \"http://www.w3.org/2001/XMLSchema#date\" , } ], \"http://schema.org/softwareVersion\" : [ { \"@value\" : self . metric_version , \"@type\" : \"http://www.w3.org/2001/XMLSchema#float\" , } ], \"http://schema.org/comment\" : [{ \"@value\" : \" \\n\\n \" . join ( self . comment ), \"@language\" : \"en\" }], \"http://semanticscience.org/resource/SIO_000332\" : [{ \"@value\" : str ( self . subject ), \"@language\" : \"en\" }], \"http://semanticscience.org/resource/SIO_000300\" : [ { \"@value\" : float ( self . score ), \"@type\" : \"http://www.w3.org/2001/XMLSchema#float\" , } ], \"http://semanticscience.org/resource/metadata\" : self . data , } ] # Logging utilities def log ( self , log_msg : str , prefix : Optional [ str ] = None ) -> None : # Add timestamp? log_msg = \"[\" + str ( datetime . datetime . now () . strftime ( \"%Y-%m- %d T%H:%M:%S\" )) + \"] \" + log_msg if prefix : log_msg = prefix + \" \" + log_msg self . comment . append ( log_msg ) # print(log_msg) def warn ( self , log_msg : str ) -> None : \"\"\" Log a warning related to the FAIR test execution (add to the comments of the test) Parameters: log_msg: Message to log \"\"\" self . log ( log_msg , \"WARN:\" ) def info ( self , log_msg : str ) -> None : \"\"\" Log an info message related to the FAIR test execution (add to the comments of the test) Parameters: log_msg: Message to log \"\"\" self . log ( log_msg , \"INFO:\" ) def failure ( self , log_msg : str ) -> None : \"\"\" Log a failure message related to the FAIR test execution (add to the comments of the test and set score to 0) Parameters: log_msg: Message to log \"\"\" self . score = 0 self . log ( log_msg , \"FAILURE:\" ) def success ( self , log_msg : str ) -> None : \"\"\" Log a success message related to the FAIR test execution (add to the comments of the test and set score to 1) Parameters: log_msg: Message to log \"\"\" if self . score >= 1 : self . bonus ( log_msg ) else : self . score += 1 self . log ( log_msg , \"SUCCESS:\" ) def bonus ( self , log_msg : str ) -> None : self . score_bonus += 1 self . log ( log_msg , \"SUCCESS:\" ) extract_data_subject ( g , subject_uri = None ) Helper to easily retrieve the subject URI of the data from RDF metadata (RDFLib Graph) Parameters: Name Type Description Default g Graph RDFLib Graph required subject_uri Optional [ List [ Any ]] metadata subject URI None Returns: Name Type Description data_uri list List of URI found for the data in the metadata Source code in fair_test/fair_test_evaluation.py 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 def extract_data_subject ( self , g : Any , subject_uri : Optional [ List [ Any ]] = None ) -> List [ Any ]: \"\"\" Helper to easily retrieve the subject URI of the data from RDF metadata (RDFLib Graph) Parameters: g (Graph): RDFLib Graph subject_uri: metadata subject URI Returns: data_uri (list): List of URI found for the data in the metadata \"\"\" data_props = [ \"https://www.w3.org/ns/ldp#contains\" , \"https://xmlns.com/foaf/0.1/primaryTopic\" , \"https://schema.org/about\" , \"https://schema.org/mainEntity\" , \"https://schema.org/codeRepository\" , \"https://schema.org/distribution\" , \"https://www.w3.org/ns/dcat#distribution\" , \"https://semanticscience.org/resource/SIO_000332\" , \"https://semanticscience.org/resource/is-about\" , \"https://purl.obolibrary.org/obo/IAO_0000136\" , ] # http_props = [p.replace('https://', 'http://') for p in data_props] if not subject_uri : subject_uri = [ URIRef ( str ( s )) for s in self . data [ \"alternative_uris\" ]] self . info ( f \"Searching for the data URI using the following predicates: { ', ' . join ( data_props ) } \" ) data_uris = self . extract_prop ( g , preds = data_props , subj = subject_uri ) # Also extract data download URL when possible content_props = [ \"https://schema.org/url\" , \"https://schema.org/contentUrl\" , \"http://www.w3.org/ns/dcat#downloadURL\" , ] self . info ( f \"Checking if the data URI point to a download URL using one of the following predicates: { ', ' . join ( content_props ) } \" ) extracted_urls = set () for data_uri in data_uris : if isinstance ( data_uri , BNode ): content_urls = self . extract_prop ( g , preds = content_props , subj = data_uri ) for content_url in content_urls : extracted_urls . add ( str ( content_url )) else : extracted_urls . add ( str ( data_uri )) if not \"data_url\" in self . data . keys (): self . data [ \"content_url\" ] = [] self . data [ \"content_url\" ] = self . data [ \"content_url\" ] + list ( extracted_urls ) return data_uris extract_metadata_subject ( g , alt_uris = None ) Helper to extract the subject URI to which metadata about the resource is attached in a RDFLib Graph Parameters: Name Type Description Default g Graph RDFLib Graph required alt_uris Optional [ List [ str ]] List of alternative URIs for the subject to find None Returns: Name Type Description subject_uri Any The subject URI used as ID in the metadata Source code in fair_test/fair_test_evaluation.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def extract_metadata_subject ( self , g : Any , alt_uris : Optional [ List [ str ]] = None ) -> Any : \"\"\" Helper to extract the subject URI to which metadata about the resource is attached in a RDFLib Graph Parameters: g (Graph): RDFLib Graph alt_uris: List of alternative URIs for the subject to find Returns: subject_uri: The subject URI used as ID in the metadata \"\"\" subject_uri = None if not alt_uris : alt_uris = self . data [ \"alternative_uris\" ] preds_id = [ \"https://purl.org/dc/terms/identifier\" , \"https://purl.org/dc/elements/1.1/identifier\" , \"https://schema.org/identifier\" , \"https://schema.org/sameAs\" , \"http://ogp.me/ns#url\" , ] all_preds_id = [ p . replace ( \"https://\" , \"http://\" ) for p in preds_id ] + preds_id all_preds_uris = [ URIRef ( str ( s )) for s in all_preds_id ] resource_properties = {} resource_linked_to = {} for alt_uri in alt_uris : uri_ref = URIRef ( str ( alt_uri )) # Search with the subject URI as triple subject for s , p , o in g . triples (( uri_ref , None , None )): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_properties [ str ( p )] = str ( o ) subject_uri = uri_ref if not subject_uri : # Search with the subject URI as triple object for pred in all_preds_uris : for s , p , o in g . triples (( None , pred , uri_ref )): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_linked_to [ str ( s )] = str ( p ) subject_uri = s if not subject_uri : # Also check when the subject URI defined as Literal for s , p , o in g . triples (( None , pred , Literal ( str ( uri_ref )))): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_linked_to [ str ( s )] = str ( p ) subject_uri = s if len ( resource_properties . keys ()) > 0 or len ( resource_linked_to . keys ()) > 0 : if not \"identifier_in_metadata\" in self . data . keys (): self . data [ \"identifier_in_metadata\" ] = {} if len ( resource_properties . keys ()) > 0 : self . data [ \"identifier_in_metadata\" ][ \"properties\" ] = resource_properties if len ( resource_linked_to . keys ()) > 0 : self . data [ \"identifier_in_metadata\" ][ \"linked_to\" ] = resource_linked_to return subject_uri extract_prop ( g , preds , subj = None ) Helper to extract properties from a RDFLib Graph Parameters: Name Type Description Default g Graph RDFLib Graph required preds List [ Any ] List of predicates to find value for required subj Optional [ Any ] Optionally also limit the results for a list of subjects None Returns: Name Type Description props List [ Any ] A list of the values found for the given properties Source code in fair_test/fair_test_evaluation.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 def extract_prop ( self , g : Any , preds : List [ Any ], subj : Optional [ Any ] = None ) -> List [ Any ]: \"\"\" Helper to extract properties from a RDFLib Graph Parameters: g (Graph): RDFLib Graph preds: List of predicates to find value for subj: Optionally also limit the results for a list of subjects Returns: props: A list of the values found for the given properties \"\"\" values = set () check_preds = set () for pred in preds : # Add the http/https counterpart for each predicate check_preds . add ( URIRef ( str ( pred ))) if str ( pred ) . startswith ( \"http://\" ): check_preds . add ( URIRef ( str ( pred ) . replace ( \"http://\" , \"https://\" ))) elif str ( pred ) . startswith ( \"https://\" ): check_preds . add ( URIRef ( str ( pred ) . replace ( \"https://\" , \"http://\" ))) # self.info(f\"Checking properties values for properties: {preds}\") # if subj: # self.info(f\"Checking properties values for subject URI(s): {str(subj)}\") for pred in list ( check_preds ): if not isinstance ( subj , list ): subj = [ subj ] # test_subjs = [URIRef(str(s)) for s in subj] for test_subj in subj : for s , p , o in g . triples (( test_subj , URIRef ( str ( pred )), None )): self . info ( f \"Found a value for a property { str ( pred ) } => { str ( o ) } \" ) values . add ( o ) return list ( values ) failure ( log_msg ) Log a failure message related to the FAIR test execution (add to the comments of the test and set score to 0) Parameters: Name Type Description Default log_msg str Message to log required Source code in fair_test/fair_test_evaluation.py 576 577 578 579 580 581 582 583 584 def failure ( self , log_msg : str ) -> None : \"\"\" Log a failure message related to the FAIR test execution (add to the comments of the test and set score to 0) Parameters: log_msg: Message to log \"\"\" self . score = 0 self . log ( log_msg , \"FAILURE:\" ) get_url ( id ) Return the full URL for a given identifiers (e.g. URL, DOI, handle) Source code in fair_test/fair_test_evaluation.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def get_url ( self , id : str ) -> str : \"\"\"Return the full URL for a given identifiers (e.g. URL, DOI, handle)\"\"\" if idutils . is_url ( id ): self . info ( f \"Validated the resource { id } is a URL\" ) return id if idutils . is_doi ( id ): self . info ( f \"Validated the resource { id } is a DOI\" ) return idutils . to_url ( id , \"doi\" , \"https\" ) if idutils . is_handle ( id ): self . info ( f \"Validated the resource { id } is a handle\" ) return idutils . to_url ( id , \"handle\" , \"https\" ) # TODO: add INCHI key? # inchikey = regexp(/^\\w{14}\\-\\w{10}\\-\\w$/) # return f\"https://pubchem.ncbi.nlm.nih.gov/rest/rdf/inchikey/{inchikey}\" self . warn ( f \"Could not validate the given resource URI { id } is a URL, DOI, or handle\" ) return None info ( log_msg ) Log an info message related to the FAIR test execution (add to the comments of the test) Parameters: Name Type Description Default log_msg str Message to log required Source code in fair_test/fair_test_evaluation.py 567 568 569 570 571 572 573 574 def info ( self , log_msg : str ) -> None : \"\"\" Log an info message related to the FAIR test execution (add to the comments of the test) Parameters: log_msg: Message to log \"\"\" self . log ( log_msg , \"INFO:\" ) parse_rdf ( rdf_data , mime_type = None , log_msg = '' ) Parse any string or JSON-like object to a RDFLib Graph Parameters: Name Type Description Default rdf_data str | object Text or object to convert to RDF required mime_type Optional [ str ] Mime type of the data to convert None log_msg Optional [ str ] Text to use when logging about the parsing process (help debugging) '' Returns: Name Type Description g Graph A RDFLib Graph Source code in fair_test/fair_test_evaluation.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def parse_rdf ( self , rdf_data : Any , mime_type : Optional [ str ] = None , log_msg : Optional [ str ] = \"\" , ) -> Any : \"\"\" Parse any string or JSON-like object to a RDFLib Graph Parameters: rdf_data (str|object): Text or object to convert to RDF mime_type: Mime type of the data to convert log_msg: Text to use when logging about the parsing process (help debugging) Returns: g (Graph): A RDFLib Graph \"\"\" # https://rdflib.readthedocs.io/en/stable/plugin_parsers.html parse_formats = [ \"turtle\" , \"json-ld\" , \"xml\" , \"ntriples\" , \"nquads\" , \"trig\" , \"n3\" ] if type ( rdf_data ) == dict : rdf_data = [ rdf_data ] if type ( rdf_data ) == list : for rdf_entry in rdf_data : try : # Dirty hack to fix RDFLib that is not able to parse JSON-LD schema.org (https://github.com/schemaorg/schemaorg/issues/2578) if \"@context\" in rdf_entry : if isinstance ( rdf_entry [ \"@context\" ], str ): if rdf_entry [ \"@context\" ] . startswith ( \"http://schema.org\" ) or rdf_entry [ \"@context\" ] . startswith ( \"https://schema.org\" ): rdf_entry [ \"@context\" ] = \"https://schema.org/docs/jsonldcontext.json\" if isinstance ( rdf_entry [ \"@context\" ], list ): for i , cont in enumerate ( rdf_entry [ \"@context\" ]): if isinstance ( cont , str ): rdf_entry [ \"@context\" ][ i ] = \"https://schema.org/docs/jsonldcontext.json\" except : pass # RDFLib JSON-LD had issue with encoding: https://github.com/RDFLib/rdflib/issues/1416 rdf_data = jsonld . expand ( rdf_data ) rdf_data = json . dumps ( rdf_data ) parse_formats = [ \"json-ld\" ] else : # Try to guess the format to parse from mime type mime_type = mime_type . split ( \";\" )[ 0 ] if \"turtle\" in mime_type : parse_formats = [ \"turtle\" ] elif \"xml\" in mime_type : parse_formats = [ \"xml\" ] elif \"ntriples\" in mime_type : parse_formats = [ \"ntriples\" ] elif \"nquads\" in mime_type : parse_formats = [ \"nquads\" ] elif \"trig\" in mime_type : parse_formats = [ \"trig\" ] # elif mime_type.startswith('text/html'): # parse_formats = [] g = ConjunctiveGraph () # Remove some auto-generated triples about the HTML content remove_preds = [ \"http://www.w3.org/1999/xhtml/vocab#role\" ] for rdf_format in parse_formats : try : g = ConjunctiveGraph () g . parse ( data = rdf_data , format = rdf_format ) for rm_pred in remove_preds : g . remove (( None , URIRef ( rm_pred ), None )) self . info ( f \"Successfully parsed { mime_type } RDF from { log_msg } with parser { rdf_format } , containing { str ( len ( g )) } triples\" ) return g except Exception as e : self . info ( f \"Could not parse { mime_type } metadata from { log_msg } with parser { rdf_format } . Getting error: { str ( e ) } \" ) return g response () Function used to generate the FAIR metric test results as JSON-LD, and return this JSON-LD as HTTP response Returns: Name Type Description response JSONResponse HTTP response containing the test results as JSON-LD Source code in fair_test/fair_test_evaluation.py 509 510 511 512 513 514 515 516 def response ( self ) -> JSONResponse : \"\"\" Function used to generate the FAIR metric test results as JSON-LD, and return this JSON-LD as HTTP response Returns: response: HTTP response containing the test results as JSON-LD \"\"\" return JSONResponse ( self . to_jsonld ()) retrieve_metadata ( url , use_harvester = False , harvester_url = 'https://fair-tests.137.120.31.101.nip.io/tests/harvester' ) Retrieve metadata from a URL, RDF metadata parsed as a RDFLib Graph in priority. Super useful. It tries: - Following signposting links (returned in HTTP headers) - Extracting JSON-LD embedded in the HTML - Asking RDF through content-negociation - Can return JSON found as a fallback, if RDF metadata is not found You can also use an external harvester API to get the RDF metadata Parameters: Name Type Description Default url str URL to retrieve RDF from required use_harvester Optional [ bool ] Use an external harvester to retrieve the RDF instead of the built-in python harvester False harvester_url Optional [ str ] URL of the RDF harvester used 'https://fair-tests.137.120.31.101.nip.io/tests/harvester' Returns: Name Type Description g Graph A RDFLib Graph with the RDF found at the given URL Source code in fair_test/fair_test_evaluation.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def retrieve_metadata ( self , url : str , use_harvester : Optional [ bool ] = False , harvester_url : Optional [ str ] = \"https://fair-tests.137.120.31.101.nip.io/tests/harvester\" , ) -> Any : \"\"\" Retrieve metadata from a URL, RDF metadata parsed as a RDFLib Graph in priority. Super useful. It tries: - Following signposting links (returned in HTTP headers) - Extracting JSON-LD embedded in the HTML - Asking RDF through content-negociation - Can return JSON found as a fallback, if RDF metadata is not found You can also use an external harvester API to get the RDF metadata Parameters: url: URL to retrieve RDF from use_harvester: Use an external harvester to retrieve the RDF instead of the built-in python harvester harvester_url: URL of the RDF harvester used Returns: g (Graph): A RDFLib Graph with the RDF found at the given URL \"\"\" original_url = url url = self . get_url ( url ) if not url : self . warn ( f \"The resource { original_url } could not be converted to a valid URL, hence no metadata could be retrieved\" ) return [] if use_harvester == True : # Check the harvester response: # curl -X POST -d '{\"subject\": \"https://doi.org/10.1594/PANGAEA.908011\"}' https://fair-tests.137.120.31.101.nip.io/tests/harvester try : self . info ( f \"Using Harvester at { harvester_url } to retrieve RDF metadata at { url } \" ) res = requests . post ( harvester_url , json = { \"subject\" : url }, timeout = 60 , # headers={\"Accept\": \"application/ld+json\"} ) return self . parse_rdf ( res . text , \"text/turtle\" , log_msg = \"FAIR evaluator harvester RDF\" ) except Exception : self . warn ( f \"Failed to reach the Harvester at { harvester_url } , using the built-in python harvester\" ) # https://github.com/FAIRMetrics/Metrics/blob/master/MetricsEvaluatorCode/Ruby/metrictests/fair_metrics_utilities.rb#L355 html_text = None metadata_obj = [] # Check if URL resolve and if redirection # r = requests.head(url) try : r = requests . get ( url ) r . raise_for_status () # Raises a HTTPError if the status is 4xx, 5xxx self . info ( f \"Successfully resolved { url } \" ) html_text = r . text if r . history : # Extract alternative URIs if request redirected redirect_url = r . url if redirect_url . startswith ( \"https://linkinghub.elsevier.com/retrieve/pii/\" ): # Special case to handle Elsevier bad redirections to ScienceDirect redirect_url = redirect_url . replace ( \"https://linkinghub.elsevier.com/retrieve/pii/\" , \"https://www.sciencedirect.com/science/article/pii/\" , ) self . data [ \"redirect_url\" ] = redirect_url if url == self . subject and not redirect_url in self . data [ \"alternative_uris\" ]: self . info ( f \"Request was redirected to { redirect_url } , adding to the list of alternative URIs for the subject\" ) self . data [ \"alternative_uris\" ] . append ( redirect_url ) if r . url . startswith ( \"http://\" ): self . data [ \"alternative_uris\" ] . append ( redirect_url . replace ( \"http://\" , \"https://\" )) elif r . url . startswith ( \"https://\" ): self . data [ \"alternative_uris\" ] . append ( redirect_url . replace ( \"https://\" , \"http://\" )) # Handle signposting links headers https://signposting.org/FAIR if r . links : # Follow signposting links, this could create a lot of recursions (to be checked) self . info ( f \"Found Signposting links: { str ( r . links ) } \" ) self . data [ \"signposting_links\" ] = r . links check_rels = [ \"alternate\" , \"describedby\" , \"meta\" ] # Alternate is used by schema.org for rel in check_rels : if rel in r . links . keys (): rel_url = r . links [ rel ][ \"url\" ] if not rel_url . startswith ( \"http://\" ) and not rel_url . startswith ( \"https://\" ): # In some case the rel URL provided is relative to the requested URL if r . url . endswith ( \"/\" ) and rel_url . startswith ( \"/\" ): rel_url = rel_url [ 1 :] rel_url = r . url + rel_url metadata_obj = self . retrieve_metadata ( rel_url ) if len ( metadata_obj ) > 0 : return metadata_obj except Exception as e : self . warn ( f \"Error resolving the URL { url } : { str ( e . args [ 0 ]) } \" ) self . info ( \"Checking for metadata embedded in the HTML page returned by the resource URI \" + url + \" using extruct\" ) # TODO: support client-side JS generated HTML using Selenium https://github.com/vemonet/extruct-selenium try : extructed = extruct . extract ( html_text . encode ( \"utf8\" )) if url == self . subject : self . data [ \"extruct\" ] = extructed if len ( extructed [ \"json-ld\" ]) > 0 : g = self . parse_rdf ( extructed [ \"json-ld\" ], \"json-ld\" , log_msg = \"HTML embedded JSON-LD RDF\" ) if len ( g ) > 0 : self . info ( f \"Found JSON-LD RDF metadata embedded in the HTML with extruct\" ) return g else : metadata_obj = extructed [ \"json-ld\" ] if len ( extructed [ \"rdfa\" ]) > 0 : g = self . parse_rdf ( extructed [ \"rdfa\" ], \"json-ld\" , log_msg = \"HTML embedded RDFa\" ) if len ( g ) > 0 : self . info ( f \"Found RDFa metadata embedded in the HTML with extruct\" ) return g elif not metadata_obj : metadata_obj = extructed [ \"rdfa\" ] if not metadata_obj and len ( extructed [ \"microdata\" ]) > 0 : metadata_obj = extructed [ \"microdata\" ] if not metadata_obj and extructed [ \"dublincore\" ] != [{ \"namespaces\" : {}, \"elements\" : [], \"terms\" : []}]: # Dublin core always comes as this empty dict if no match metadata_obj = extructed [ \"dublincore\" ] # The rest is not extracted because usually give no interesting metadata: # opengraph, microformat except Exception as e : self . info ( f \"Error when running extruct on { url } . Getting: { str ( e . args [ 0 ]) } \" ) # Perform content negociation last because it's the slowest for a lot of URLs like zenodo # We need to do direct content negociation to turtle and json # because some URLs dont support standard weighted content negociation check_mime_types = [ \"text/turtle\" , \"application/ld+json\" , \"text/turtle, application/turtle, application/x-turtle;q=0.9, application/ld+json;q=0.8, application/rdf+xml, text/n3, text/rdf+n3;q=0.7\" , ] for mime_type in check_mime_types : try : r = requests . get ( url , headers = { \"accept\" : mime_type }) r . raise_for_status () # Raises a HTTPError if the status is 4xx, 5xxx content_type = r . headers [ \"Content-Type\" ] . replace ( \" \" , \"\" ) . replace ( \";charset=utf-8\" , \"\" ) # If return text/plain we parse as turtle or JSON-LD # content_type = content_type.replace('text/plain', 'text/turtle') self . info ( f \"Content-negotiation: found some metadata in { content_type } when asking for { mime_type } \" ) try : # If returns JSON self . data [ \"json-ld\" ] = r . json () if not metadata_obj : metadata_obj = r . json () return self . parse_rdf ( r . json (), \"json-ld\" , log_msg = \"content negotiation JSON-LD RDF\" ) except Exception : # If returns RDF as text, such as turtle return self . parse_rdf ( r . text , content_type , log_msg = \"content negotiation RDF\" ) except Exception as e : self . info ( f \"Content-negotiation: error with { url } when asking for { mime_type } . Getting { str ( e . args [ 0 ]) } \" ) # Error: e.args[0] return metadata_obj success ( log_msg ) Log a success message related to the FAIR test execution (add to the comments of the test and set score to 1) Parameters: Name Type Description Default log_msg str Message to log required Source code in fair_test/fair_test_evaluation.py 586 587 588 589 590 591 592 593 594 595 596 597 def success ( self , log_msg : str ) -> None : \"\"\" Log a success message related to the FAIR test execution (add to the comments of the test and set score to 1) Parameters: log_msg: Message to log \"\"\" if self . score >= 1 : self . bonus ( log_msg ) else : self . score += 1 self . log ( log_msg , \"SUCCESS:\" ) warn ( log_msg ) Log a warning related to the FAIR test execution (add to the comments of the test) Parameters: Name Type Description Default log_msg str Message to log required Source code in fair_test/fair_test_evaluation.py 558 559 560 561 562 563 564 565 def warn ( self , log_msg : str ) -> None : \"\"\" Log a warning related to the FAIR test execution (add to the comments of the test) Parameters: log_msg: Message to log \"\"\" self . log ( log_msg , \"WARN:\" )","title":"<span><i class='fa-solid fa-vial-circle-check'></i>&nbsp;&nbsp;FairTestEvaluation</span>"},{"location":"FairTestEvaluation/#fairtestevaluation","text":"Bases: BaseModel Class to manipulate a FAIR metrics test evaluation. Provides helpers functions to easily retrieve and parse metadata. A new FairTestEvaluation object is create for each new request to one of the FAIR metrics test API call exposed by the API metrics/a1_check_something.py from fair_test import FairTest , FairTestEvaluation class MetricTest ( FairTest ): metric_path = 'a1-check-something' applies_to_principle = 'A1' title = 'Check something' description = \"Test something\" author = 'https://orcid.org/0000-0000-0000-0000' metric_version = '0.1.0' test_test = { 'http://doi.org/10.1594/PANGAEA.908011' : 1 , 'https://github.com/MaastrichtU-IDS/fair-test' : 0 , } def evaluate ( self , eval : FairTestEvaluation ): eval . info ( f 'Checking something for { self . subject } ' ) g = eval . retrieve_metadata ( self . subject , use_harvester = False ) if len ( g ) > 0 : eval . success ( f ' { len ( g ) } triples found, test sucessful' ) else : eval . failure ( 'No triples found, test failed' ) return eval . response () Source code in fair_test/fair_test_evaluation.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 class FairTestEvaluation ( BaseModel ): \"\"\" Class to manipulate a FAIR metrics test evaluation. Provides helpers functions to easily retrieve and parse metadata. A new FairTestEvaluation object is create for each new request to one of the FAIR metrics test API call exposed by the API ```python title=\"metrics/a1_check_something.py\" from fair_test import FairTest, FairTestEvaluation class MetricTest(FairTest): metric_path = 'a1-check-something' applies_to_principle = 'A1' title = 'Check something' description = \"Test something\" author = 'https://orcid.org/0000-0000-0000-0000' metric_version = '0.1.0' test_test={ 'http://doi.org/10.1594/PANGAEA.908011': 1, 'https://github.com/MaastrichtU-IDS/fair-test': 0, } def evaluate(self, eval: FairTestEvaluation): eval.info(f'Checking something for {self.subject}') g = eval.retrieve_metadata(self.subject, use_harvester=False) if len(g) > 0: eval.success(f'{len(g)} triples found, test sucessful') else: eval.failure('No triples found, test failed') return eval.response() ``` \"\"\" subject : Optional [ str ] subject_url : Optional [ str ] score : int = 0 score_bonus : int = 0 comment : List = [] date : str = datetime . datetime . now () . strftime ( \"%Y-%m- %d T%H:%M:%S+01:00\" ) metric_version : str = \"0.1.0\" id : Optional [ str ] # URL of the test results data : Optional [ dict ] = {} def __init__ ( self , subject : str , metric_path : str ) -> None : super () . __init__ () self . subject = subject self . id = f \" { settings . HOST_URL } /metrics/ { metric_path } # { quote ( str ( self . subject )) } /result- { self . date } \" self . subject_url = self . get_url ( subject ) # Add potential alternative URIs for the subject if self . subject_url : alt_uris = set () alt_uris . add ( self . subject_url ) # Add HTTPS/HTTPS counterpart as alternative URIs if self . subject_url . startswith ( \"http://\" ): alt_uris . add ( self . subject_url . replace ( \"http://\" , \"https://\" )) elif self . subject . startswith ( \"https://\" ): alt_uris . add ( self . subject_url . replace ( \"https://\" , \"http://\" )) # Fix to add an alternative URI for doi.org that is commonly used as identifier in the metadata parsed_url = urlparse ( self . subject_url ) if parsed_url . netloc and parsed_url . netloc == \"doi.org\" : alt_uris . add ( \"http://dx.doi.org/\" + parsed_url . path [ 1 :]) self . data [ \"alternative_uris\" ] = list ( alt_uris ) class Config : arbitrary_types_allowed = True def get_url ( self , id : str ) -> str : \"\"\"Return the full URL for a given identifiers (e.g. URL, DOI, handle)\"\"\" if idutils . is_url ( id ): self . info ( f \"Validated the resource { id } is a URL\" ) return id if idutils . is_doi ( id ): self . info ( f \"Validated the resource { id } is a DOI\" ) return idutils . to_url ( id , \"doi\" , \"https\" ) if idutils . is_handle ( id ): self . info ( f \"Validated the resource { id } is a handle\" ) return idutils . to_url ( id , \"handle\" , \"https\" ) # TODO: add INCHI key? # inchikey = regexp(/^\\w{14}\\-\\w{10}\\-\\w$/) # return f\"https://pubchem.ncbi.nlm.nih.gov/rest/rdf/inchikey/{inchikey}\" self . warn ( f \"Could not validate the given resource URI { id } is a URL, DOI, or handle\" ) return None # TODO: implement metadata extraction with more tools? # e.g. Apache Tika for PDF/pptx? or ruby Kellog's Distiller? http://rdf.greggkellogg.net/distiller # c.f. https://github.com/FAIRMetrics/Metrics/blob/master/MetricsEvaluatorCode/Ruby/metrictests/fair_metrics_utilities.rb def retrieve_metadata ( self , url : str , use_harvester : Optional [ bool ] = False , harvester_url : Optional [ str ] = \"https://fair-tests.137.120.31.101.nip.io/tests/harvester\" , ) -> Any : \"\"\" Retrieve metadata from a URL, RDF metadata parsed as a RDFLib Graph in priority. Super useful. It tries: - Following signposting links (returned in HTTP headers) - Extracting JSON-LD embedded in the HTML - Asking RDF through content-negociation - Can return JSON found as a fallback, if RDF metadata is not found You can also use an external harvester API to get the RDF metadata Parameters: url: URL to retrieve RDF from use_harvester: Use an external harvester to retrieve the RDF instead of the built-in python harvester harvester_url: URL of the RDF harvester used Returns: g (Graph): A RDFLib Graph with the RDF found at the given URL \"\"\" original_url = url url = self . get_url ( url ) if not url : self . warn ( f \"The resource { original_url } could not be converted to a valid URL, hence no metadata could be retrieved\" ) return [] if use_harvester == True : # Check the harvester response: # curl -X POST -d '{\"subject\": \"https://doi.org/10.1594/PANGAEA.908011\"}' https://fair-tests.137.120.31.101.nip.io/tests/harvester try : self . info ( f \"Using Harvester at { harvester_url } to retrieve RDF metadata at { url } \" ) res = requests . post ( harvester_url , json = { \"subject\" : url }, timeout = 60 , # headers={\"Accept\": \"application/ld+json\"} ) return self . parse_rdf ( res . text , \"text/turtle\" , log_msg = \"FAIR evaluator harvester RDF\" ) except Exception : self . warn ( f \"Failed to reach the Harvester at { harvester_url } , using the built-in python harvester\" ) # https://github.com/FAIRMetrics/Metrics/blob/master/MetricsEvaluatorCode/Ruby/metrictests/fair_metrics_utilities.rb#L355 html_text = None metadata_obj = [] # Check if URL resolve and if redirection # r = requests.head(url) try : r = requests . get ( url ) r . raise_for_status () # Raises a HTTPError if the status is 4xx, 5xxx self . info ( f \"Successfully resolved { url } \" ) html_text = r . text if r . history : # Extract alternative URIs if request redirected redirect_url = r . url if redirect_url . startswith ( \"https://linkinghub.elsevier.com/retrieve/pii/\" ): # Special case to handle Elsevier bad redirections to ScienceDirect redirect_url = redirect_url . replace ( \"https://linkinghub.elsevier.com/retrieve/pii/\" , \"https://www.sciencedirect.com/science/article/pii/\" , ) self . data [ \"redirect_url\" ] = redirect_url if url == self . subject and not redirect_url in self . data [ \"alternative_uris\" ]: self . info ( f \"Request was redirected to { redirect_url } , adding to the list of alternative URIs for the subject\" ) self . data [ \"alternative_uris\" ] . append ( redirect_url ) if r . url . startswith ( \"http://\" ): self . data [ \"alternative_uris\" ] . append ( redirect_url . replace ( \"http://\" , \"https://\" )) elif r . url . startswith ( \"https://\" ): self . data [ \"alternative_uris\" ] . append ( redirect_url . replace ( \"https://\" , \"http://\" )) # Handle signposting links headers https://signposting.org/FAIR if r . links : # Follow signposting links, this could create a lot of recursions (to be checked) self . info ( f \"Found Signposting links: { str ( r . links ) } \" ) self . data [ \"signposting_links\" ] = r . links check_rels = [ \"alternate\" , \"describedby\" , \"meta\" ] # Alternate is used by schema.org for rel in check_rels : if rel in r . links . keys (): rel_url = r . links [ rel ][ \"url\" ] if not rel_url . startswith ( \"http://\" ) and not rel_url . startswith ( \"https://\" ): # In some case the rel URL provided is relative to the requested URL if r . url . endswith ( \"/\" ) and rel_url . startswith ( \"/\" ): rel_url = rel_url [ 1 :] rel_url = r . url + rel_url metadata_obj = self . retrieve_metadata ( rel_url ) if len ( metadata_obj ) > 0 : return metadata_obj except Exception as e : self . warn ( f \"Error resolving the URL { url } : { str ( e . args [ 0 ]) } \" ) self . info ( \"Checking for metadata embedded in the HTML page returned by the resource URI \" + url + \" using extruct\" ) # TODO: support client-side JS generated HTML using Selenium https://github.com/vemonet/extruct-selenium try : extructed = extruct . extract ( html_text . encode ( \"utf8\" )) if url == self . subject : self . data [ \"extruct\" ] = extructed if len ( extructed [ \"json-ld\" ]) > 0 : g = self . parse_rdf ( extructed [ \"json-ld\" ], \"json-ld\" , log_msg = \"HTML embedded JSON-LD RDF\" ) if len ( g ) > 0 : self . info ( f \"Found JSON-LD RDF metadata embedded in the HTML with extruct\" ) return g else : metadata_obj = extructed [ \"json-ld\" ] if len ( extructed [ \"rdfa\" ]) > 0 : g = self . parse_rdf ( extructed [ \"rdfa\" ], \"json-ld\" , log_msg = \"HTML embedded RDFa\" ) if len ( g ) > 0 : self . info ( f \"Found RDFa metadata embedded in the HTML with extruct\" ) return g elif not metadata_obj : metadata_obj = extructed [ \"rdfa\" ] if not metadata_obj and len ( extructed [ \"microdata\" ]) > 0 : metadata_obj = extructed [ \"microdata\" ] if not metadata_obj and extructed [ \"dublincore\" ] != [{ \"namespaces\" : {}, \"elements\" : [], \"terms\" : []}]: # Dublin core always comes as this empty dict if no match metadata_obj = extructed [ \"dublincore\" ] # The rest is not extracted because usually give no interesting metadata: # opengraph, microformat except Exception as e : self . info ( f \"Error when running extruct on { url } . Getting: { str ( e . args [ 0 ]) } \" ) # Perform content negociation last because it's the slowest for a lot of URLs like zenodo # We need to do direct content negociation to turtle and json # because some URLs dont support standard weighted content negociation check_mime_types = [ \"text/turtle\" , \"application/ld+json\" , \"text/turtle, application/turtle, application/x-turtle;q=0.9, application/ld+json;q=0.8, application/rdf+xml, text/n3, text/rdf+n3;q=0.7\" , ] for mime_type in check_mime_types : try : r = requests . get ( url , headers = { \"accept\" : mime_type }) r . raise_for_status () # Raises a HTTPError if the status is 4xx, 5xxx content_type = r . headers [ \"Content-Type\" ] . replace ( \" \" , \"\" ) . replace ( \";charset=utf-8\" , \"\" ) # If return text/plain we parse as turtle or JSON-LD # content_type = content_type.replace('text/plain', 'text/turtle') self . info ( f \"Content-negotiation: found some metadata in { content_type } when asking for { mime_type } \" ) try : # If returns JSON self . data [ \"json-ld\" ] = r . json () if not metadata_obj : metadata_obj = r . json () return self . parse_rdf ( r . json (), \"json-ld\" , log_msg = \"content negotiation JSON-LD RDF\" ) except Exception : # If returns RDF as text, such as turtle return self . parse_rdf ( r . text , content_type , log_msg = \"content negotiation RDF\" ) except Exception as e : self . info ( f \"Content-negotiation: error with { url } when asking for { mime_type } . Getting { str ( e . args [ 0 ]) } \" ) # Error: e.args[0] return metadata_obj def parse_rdf ( self , rdf_data : Any , mime_type : Optional [ str ] = None , log_msg : Optional [ str ] = \"\" , ) -> Any : \"\"\" Parse any string or JSON-like object to a RDFLib Graph Parameters: rdf_data (str|object): Text or object to convert to RDF mime_type: Mime type of the data to convert log_msg: Text to use when logging about the parsing process (help debugging) Returns: g (Graph): A RDFLib Graph \"\"\" # https://rdflib.readthedocs.io/en/stable/plugin_parsers.html parse_formats = [ \"turtle\" , \"json-ld\" , \"xml\" , \"ntriples\" , \"nquads\" , \"trig\" , \"n3\" ] if type ( rdf_data ) == dict : rdf_data = [ rdf_data ] if type ( rdf_data ) == list : for rdf_entry in rdf_data : try : # Dirty hack to fix RDFLib that is not able to parse JSON-LD schema.org (https://github.com/schemaorg/schemaorg/issues/2578) if \"@context\" in rdf_entry : if isinstance ( rdf_entry [ \"@context\" ], str ): if rdf_entry [ \"@context\" ] . startswith ( \"http://schema.org\" ) or rdf_entry [ \"@context\" ] . startswith ( \"https://schema.org\" ): rdf_entry [ \"@context\" ] = \"https://schema.org/docs/jsonldcontext.json\" if isinstance ( rdf_entry [ \"@context\" ], list ): for i , cont in enumerate ( rdf_entry [ \"@context\" ]): if isinstance ( cont , str ): rdf_entry [ \"@context\" ][ i ] = \"https://schema.org/docs/jsonldcontext.json\" except : pass # RDFLib JSON-LD had issue with encoding: https://github.com/RDFLib/rdflib/issues/1416 rdf_data = jsonld . expand ( rdf_data ) rdf_data = json . dumps ( rdf_data ) parse_formats = [ \"json-ld\" ] else : # Try to guess the format to parse from mime type mime_type = mime_type . split ( \";\" )[ 0 ] if \"turtle\" in mime_type : parse_formats = [ \"turtle\" ] elif \"xml\" in mime_type : parse_formats = [ \"xml\" ] elif \"ntriples\" in mime_type : parse_formats = [ \"ntriples\" ] elif \"nquads\" in mime_type : parse_formats = [ \"nquads\" ] elif \"trig\" in mime_type : parse_formats = [ \"trig\" ] # elif mime_type.startswith('text/html'): # parse_formats = [] g = ConjunctiveGraph () # Remove some auto-generated triples about the HTML content remove_preds = [ \"http://www.w3.org/1999/xhtml/vocab#role\" ] for rdf_format in parse_formats : try : g = ConjunctiveGraph () g . parse ( data = rdf_data , format = rdf_format ) for rm_pred in remove_preds : g . remove (( None , URIRef ( rm_pred ), None )) self . info ( f \"Successfully parsed { mime_type } RDF from { log_msg } with parser { rdf_format } , containing { str ( len ( g )) } triples\" ) return g except Exception as e : self . info ( f \"Could not parse { mime_type } metadata from { log_msg } with parser { rdf_format } . Getting error: { str ( e ) } \" ) return g # return None def extract_prop ( self , g : Any , preds : List [ Any ], subj : Optional [ Any ] = None ) -> List [ Any ]: \"\"\" Helper to extract properties from a RDFLib Graph Parameters: g (Graph): RDFLib Graph preds: List of predicates to find value for subj: Optionally also limit the results for a list of subjects Returns: props: A list of the values found for the given properties \"\"\" values = set () check_preds = set () for pred in preds : # Add the http/https counterpart for each predicate check_preds . add ( URIRef ( str ( pred ))) if str ( pred ) . startswith ( \"http://\" ): check_preds . add ( URIRef ( str ( pred ) . replace ( \"http://\" , \"https://\" ))) elif str ( pred ) . startswith ( \"https://\" ): check_preds . add ( URIRef ( str ( pred ) . replace ( \"https://\" , \"http://\" ))) # self.info(f\"Checking properties values for properties: {preds}\") # if subj: # self.info(f\"Checking properties values for subject URI(s): {str(subj)}\") for pred in list ( check_preds ): if not isinstance ( subj , list ): subj = [ subj ] # test_subjs = [URIRef(str(s)) for s in subj] for test_subj in subj : for s , p , o in g . triples (( test_subj , URIRef ( str ( pred )), None )): self . info ( f \"Found a value for a property { str ( pred ) } => { str ( o ) } \" ) values . add ( o ) return list ( values ) def extract_metadata_subject ( self , g : Any , alt_uris : Optional [ List [ str ]] = None ) -> Any : \"\"\" Helper to extract the subject URI to which metadata about the resource is attached in a RDFLib Graph Parameters: g (Graph): RDFLib Graph alt_uris: List of alternative URIs for the subject to find Returns: subject_uri: The subject URI used as ID in the metadata \"\"\" subject_uri = None if not alt_uris : alt_uris = self . data [ \"alternative_uris\" ] preds_id = [ \"https://purl.org/dc/terms/identifier\" , \"https://purl.org/dc/elements/1.1/identifier\" , \"https://schema.org/identifier\" , \"https://schema.org/sameAs\" , \"http://ogp.me/ns#url\" , ] all_preds_id = [ p . replace ( \"https://\" , \"http://\" ) for p in preds_id ] + preds_id all_preds_uris = [ URIRef ( str ( s )) for s in all_preds_id ] resource_properties = {} resource_linked_to = {} for alt_uri in alt_uris : uri_ref = URIRef ( str ( alt_uri )) # Search with the subject URI as triple subject for s , p , o in g . triples (( uri_ref , None , None )): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_properties [ str ( p )] = str ( o ) subject_uri = uri_ref if not subject_uri : # Search with the subject URI as triple object for pred in all_preds_uris : for s , p , o in g . triples (( None , pred , uri_ref )): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_linked_to [ str ( s )] = str ( p ) subject_uri = s if not subject_uri : # Also check when the subject URI defined as Literal for s , p , o in g . triples (( None , pred , Literal ( str ( uri_ref )))): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_linked_to [ str ( s )] = str ( p ) subject_uri = s if len ( resource_properties . keys ()) > 0 or len ( resource_linked_to . keys ()) > 0 : if not \"identifier_in_metadata\" in self . data . keys (): self . data [ \"identifier_in_metadata\" ] = {} if len ( resource_properties . keys ()) > 0 : self . data [ \"identifier_in_metadata\" ][ \"properties\" ] = resource_properties if len ( resource_linked_to . keys ()) > 0 : self . data [ \"identifier_in_metadata\" ][ \"linked_to\" ] = resource_linked_to return subject_uri def extract_data_subject ( self , g : Any , subject_uri : Optional [ List [ Any ]] = None ) -> List [ Any ]: \"\"\" Helper to easily retrieve the subject URI of the data from RDF metadata (RDFLib Graph) Parameters: g (Graph): RDFLib Graph subject_uri: metadata subject URI Returns: data_uri (list): List of URI found for the data in the metadata \"\"\" data_props = [ \"https://www.w3.org/ns/ldp#contains\" , \"https://xmlns.com/foaf/0.1/primaryTopic\" , \"https://schema.org/about\" , \"https://schema.org/mainEntity\" , \"https://schema.org/codeRepository\" , \"https://schema.org/distribution\" , \"https://www.w3.org/ns/dcat#distribution\" , \"https://semanticscience.org/resource/SIO_000332\" , \"https://semanticscience.org/resource/is-about\" , \"https://purl.obolibrary.org/obo/IAO_0000136\" , ] # http_props = [p.replace('https://', 'http://') for p in data_props] if not subject_uri : subject_uri = [ URIRef ( str ( s )) for s in self . data [ \"alternative_uris\" ]] self . info ( f \"Searching for the data URI using the following predicates: { ', ' . join ( data_props ) } \" ) data_uris = self . extract_prop ( g , preds = data_props , subj = subject_uri ) # Also extract data download URL when possible content_props = [ \"https://schema.org/url\" , \"https://schema.org/contentUrl\" , \"http://www.w3.org/ns/dcat#downloadURL\" , ] self . info ( f \"Checking if the data URI point to a download URL using one of the following predicates: { ', ' . join ( content_props ) } \" ) extracted_urls = set () for data_uri in data_uris : if isinstance ( data_uri , BNode ): content_urls = self . extract_prop ( g , preds = content_props , subj = data_uri ) for content_url in content_urls : extracted_urls . add ( str ( content_url )) else : extracted_urls . add ( str ( data_uri )) if not \"data_url\" in self . data . keys (): self . data [ \"content_url\" ] = [] self . data [ \"content_url\" ] = self . data [ \"content_url\" ] + list ( extracted_urls ) return data_uris def response ( self ) -> JSONResponse : \"\"\" Function used to generate the FAIR metric test results as JSON-LD, and return this JSON-LD as HTTP response Returns: response: HTTP response containing the test results as JSON-LD \"\"\" return JSONResponse ( self . to_jsonld ()) def to_jsonld ( self ) -> List [ Dict ]: # To see the object used by the original FAIR metrics: # curl -L -X 'POST' -d '{\"subject\": \"\"}' 'https://w3id.org/FAIR_Tests/tests/gen2_unique_identifier' return [ { \"@id\" : self . id , \"@type\" : [ \"http://fairmetrics.org/resources/metric_evaluation_result\" ], \"http://purl.obolibrary.org/obo/date\" : [ { \"@value\" : self . date , \"@type\" : \"http://www.w3.org/2001/XMLSchema#date\" , } ], \"http://schema.org/softwareVersion\" : [ { \"@value\" : self . metric_version , \"@type\" : \"http://www.w3.org/2001/XMLSchema#float\" , } ], \"http://schema.org/comment\" : [{ \"@value\" : \" \\n\\n \" . join ( self . comment ), \"@language\" : \"en\" }], \"http://semanticscience.org/resource/SIO_000332\" : [{ \"@value\" : str ( self . subject ), \"@language\" : \"en\" }], \"http://semanticscience.org/resource/SIO_000300\" : [ { \"@value\" : float ( self . score ), \"@type\" : \"http://www.w3.org/2001/XMLSchema#float\" , } ], \"http://semanticscience.org/resource/metadata\" : self . data , } ] # Logging utilities def log ( self , log_msg : str , prefix : Optional [ str ] = None ) -> None : # Add timestamp? log_msg = \"[\" + str ( datetime . datetime . now () . strftime ( \"%Y-%m- %d T%H:%M:%S\" )) + \"] \" + log_msg if prefix : log_msg = prefix + \" \" + log_msg self . comment . append ( log_msg ) # print(log_msg) def warn ( self , log_msg : str ) -> None : \"\"\" Log a warning related to the FAIR test execution (add to the comments of the test) Parameters: log_msg: Message to log \"\"\" self . log ( log_msg , \"WARN:\" ) def info ( self , log_msg : str ) -> None : \"\"\" Log an info message related to the FAIR test execution (add to the comments of the test) Parameters: log_msg: Message to log \"\"\" self . log ( log_msg , \"INFO:\" ) def failure ( self , log_msg : str ) -> None : \"\"\" Log a failure message related to the FAIR test execution (add to the comments of the test and set score to 0) Parameters: log_msg: Message to log \"\"\" self . score = 0 self . log ( log_msg , \"FAILURE:\" ) def success ( self , log_msg : str ) -> None : \"\"\" Log a success message related to the FAIR test execution (add to the comments of the test and set score to 1) Parameters: log_msg: Message to log \"\"\" if self . score >= 1 : self . bonus ( log_msg ) else : self . score += 1 self . log ( log_msg , \"SUCCESS:\" ) def bonus ( self , log_msg : str ) -> None : self . score_bonus += 1 self . log ( log_msg , \"SUCCESS:\" )","title":"FairTestEvaluation"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.extract_data_subject","text":"Helper to easily retrieve the subject URI of the data from RDF metadata (RDFLib Graph) Parameters: Name Type Description Default g Graph RDFLib Graph required subject_uri Optional [ List [ Any ]] metadata subject URI None Returns: Name Type Description data_uri list List of URI found for the data in the metadata Source code in fair_test/fair_test_evaluation.py 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 def extract_data_subject ( self , g : Any , subject_uri : Optional [ List [ Any ]] = None ) -> List [ Any ]: \"\"\" Helper to easily retrieve the subject URI of the data from RDF metadata (RDFLib Graph) Parameters: g (Graph): RDFLib Graph subject_uri: metadata subject URI Returns: data_uri (list): List of URI found for the data in the metadata \"\"\" data_props = [ \"https://www.w3.org/ns/ldp#contains\" , \"https://xmlns.com/foaf/0.1/primaryTopic\" , \"https://schema.org/about\" , \"https://schema.org/mainEntity\" , \"https://schema.org/codeRepository\" , \"https://schema.org/distribution\" , \"https://www.w3.org/ns/dcat#distribution\" , \"https://semanticscience.org/resource/SIO_000332\" , \"https://semanticscience.org/resource/is-about\" , \"https://purl.obolibrary.org/obo/IAO_0000136\" , ] # http_props = [p.replace('https://', 'http://') for p in data_props] if not subject_uri : subject_uri = [ URIRef ( str ( s )) for s in self . data [ \"alternative_uris\" ]] self . info ( f \"Searching for the data URI using the following predicates: { ', ' . join ( data_props ) } \" ) data_uris = self . extract_prop ( g , preds = data_props , subj = subject_uri ) # Also extract data download URL when possible content_props = [ \"https://schema.org/url\" , \"https://schema.org/contentUrl\" , \"http://www.w3.org/ns/dcat#downloadURL\" , ] self . info ( f \"Checking if the data URI point to a download URL using one of the following predicates: { ', ' . join ( content_props ) } \" ) extracted_urls = set () for data_uri in data_uris : if isinstance ( data_uri , BNode ): content_urls = self . extract_prop ( g , preds = content_props , subj = data_uri ) for content_url in content_urls : extracted_urls . add ( str ( content_url )) else : extracted_urls . add ( str ( data_uri )) if not \"data_url\" in self . data . keys (): self . data [ \"content_url\" ] = [] self . data [ \"content_url\" ] = self . data [ \"content_url\" ] + list ( extracted_urls ) return data_uris","title":"extract_data_subject()"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.extract_metadata_subject","text":"Helper to extract the subject URI to which metadata about the resource is attached in a RDFLib Graph Parameters: Name Type Description Default g Graph RDFLib Graph required alt_uris Optional [ List [ str ]] List of alternative URIs for the subject to find None Returns: Name Type Description subject_uri Any The subject URI used as ID in the metadata Source code in fair_test/fair_test_evaluation.py 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 def extract_metadata_subject ( self , g : Any , alt_uris : Optional [ List [ str ]] = None ) -> Any : \"\"\" Helper to extract the subject URI to which metadata about the resource is attached in a RDFLib Graph Parameters: g (Graph): RDFLib Graph alt_uris: List of alternative URIs for the subject to find Returns: subject_uri: The subject URI used as ID in the metadata \"\"\" subject_uri = None if not alt_uris : alt_uris = self . data [ \"alternative_uris\" ] preds_id = [ \"https://purl.org/dc/terms/identifier\" , \"https://purl.org/dc/elements/1.1/identifier\" , \"https://schema.org/identifier\" , \"https://schema.org/sameAs\" , \"http://ogp.me/ns#url\" , ] all_preds_id = [ p . replace ( \"https://\" , \"http://\" ) for p in preds_id ] + preds_id all_preds_uris = [ URIRef ( str ( s )) for s in all_preds_id ] resource_properties = {} resource_linked_to = {} for alt_uri in alt_uris : uri_ref = URIRef ( str ( alt_uri )) # Search with the subject URI as triple subject for s , p , o in g . triples (( uri_ref , None , None )): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_properties [ str ( p )] = str ( o ) subject_uri = uri_ref if not subject_uri : # Search with the subject URI as triple object for pred in all_preds_uris : for s , p , o in g . triples (( None , pred , uri_ref )): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_linked_to [ str ( s )] = str ( p ) subject_uri = s if not subject_uri : # Also check when the subject URI defined as Literal for s , p , o in g . triples (( None , pred , Literal ( str ( uri_ref )))): self . info ( f \"Found the subject URI in the metadata: { str ( s ) } \" ) resource_linked_to [ str ( s )] = str ( p ) subject_uri = s if len ( resource_properties . keys ()) > 0 or len ( resource_linked_to . keys ()) > 0 : if not \"identifier_in_metadata\" in self . data . keys (): self . data [ \"identifier_in_metadata\" ] = {} if len ( resource_properties . keys ()) > 0 : self . data [ \"identifier_in_metadata\" ][ \"properties\" ] = resource_properties if len ( resource_linked_to . keys ()) > 0 : self . data [ \"identifier_in_metadata\" ][ \"linked_to\" ] = resource_linked_to return subject_uri","title":"extract_metadata_subject()"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.extract_prop","text":"Helper to extract properties from a RDFLib Graph Parameters: Name Type Description Default g Graph RDFLib Graph required preds List [ Any ] List of predicates to find value for required subj Optional [ Any ] Optionally also limit the results for a list of subjects None Returns: Name Type Description props List [ Any ] A list of the values found for the given properties Source code in fair_test/fair_test_evaluation.py 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 def extract_prop ( self , g : Any , preds : List [ Any ], subj : Optional [ Any ] = None ) -> List [ Any ]: \"\"\" Helper to extract properties from a RDFLib Graph Parameters: g (Graph): RDFLib Graph preds: List of predicates to find value for subj: Optionally also limit the results for a list of subjects Returns: props: A list of the values found for the given properties \"\"\" values = set () check_preds = set () for pred in preds : # Add the http/https counterpart for each predicate check_preds . add ( URIRef ( str ( pred ))) if str ( pred ) . startswith ( \"http://\" ): check_preds . add ( URIRef ( str ( pred ) . replace ( \"http://\" , \"https://\" ))) elif str ( pred ) . startswith ( \"https://\" ): check_preds . add ( URIRef ( str ( pred ) . replace ( \"https://\" , \"http://\" ))) # self.info(f\"Checking properties values for properties: {preds}\") # if subj: # self.info(f\"Checking properties values for subject URI(s): {str(subj)}\") for pred in list ( check_preds ): if not isinstance ( subj , list ): subj = [ subj ] # test_subjs = [URIRef(str(s)) for s in subj] for test_subj in subj : for s , p , o in g . triples (( test_subj , URIRef ( str ( pred )), None )): self . info ( f \"Found a value for a property { str ( pred ) } => { str ( o ) } \" ) values . add ( o ) return list ( values )","title":"extract_prop()"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.failure","text":"Log a failure message related to the FAIR test execution (add to the comments of the test and set score to 0) Parameters: Name Type Description Default log_msg str Message to log required Source code in fair_test/fair_test_evaluation.py 576 577 578 579 580 581 582 583 584 def failure ( self , log_msg : str ) -> None : \"\"\" Log a failure message related to the FAIR test execution (add to the comments of the test and set score to 0) Parameters: log_msg: Message to log \"\"\" self . score = 0 self . log ( log_msg , \"FAILURE:\" )","title":"failure()"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.get_url","text":"Return the full URL for a given identifiers (e.g. URL, DOI, handle) Source code in fair_test/fair_test_evaluation.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def get_url ( self , id : str ) -> str : \"\"\"Return the full URL for a given identifiers (e.g. URL, DOI, handle)\"\"\" if idutils . is_url ( id ): self . info ( f \"Validated the resource { id } is a URL\" ) return id if idutils . is_doi ( id ): self . info ( f \"Validated the resource { id } is a DOI\" ) return idutils . to_url ( id , \"doi\" , \"https\" ) if idutils . is_handle ( id ): self . info ( f \"Validated the resource { id } is a handle\" ) return idutils . to_url ( id , \"handle\" , \"https\" ) # TODO: add INCHI key? # inchikey = regexp(/^\\w{14}\\-\\w{10}\\-\\w$/) # return f\"https://pubchem.ncbi.nlm.nih.gov/rest/rdf/inchikey/{inchikey}\" self . warn ( f \"Could not validate the given resource URI { id } is a URL, DOI, or handle\" ) return None","title":"get_url()"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.info","text":"Log an info message related to the FAIR test execution (add to the comments of the test) Parameters: Name Type Description Default log_msg str Message to log required Source code in fair_test/fair_test_evaluation.py 567 568 569 570 571 572 573 574 def info ( self , log_msg : str ) -> None : \"\"\" Log an info message related to the FAIR test execution (add to the comments of the test) Parameters: log_msg: Message to log \"\"\" self . log ( log_msg , \"INFO:\" )","title":"info()"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.parse_rdf","text":"Parse any string or JSON-like object to a RDFLib Graph Parameters: Name Type Description Default rdf_data str | object Text or object to convert to RDF required mime_type Optional [ str ] Mime type of the data to convert None log_msg Optional [ str ] Text to use when logging about the parsing process (help debugging) '' Returns: Name Type Description g Graph A RDFLib Graph Source code in fair_test/fair_test_evaluation.py 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 def parse_rdf ( self , rdf_data : Any , mime_type : Optional [ str ] = None , log_msg : Optional [ str ] = \"\" , ) -> Any : \"\"\" Parse any string or JSON-like object to a RDFLib Graph Parameters: rdf_data (str|object): Text or object to convert to RDF mime_type: Mime type of the data to convert log_msg: Text to use when logging about the parsing process (help debugging) Returns: g (Graph): A RDFLib Graph \"\"\" # https://rdflib.readthedocs.io/en/stable/plugin_parsers.html parse_formats = [ \"turtle\" , \"json-ld\" , \"xml\" , \"ntriples\" , \"nquads\" , \"trig\" , \"n3\" ] if type ( rdf_data ) == dict : rdf_data = [ rdf_data ] if type ( rdf_data ) == list : for rdf_entry in rdf_data : try : # Dirty hack to fix RDFLib that is not able to parse JSON-LD schema.org (https://github.com/schemaorg/schemaorg/issues/2578) if \"@context\" in rdf_entry : if isinstance ( rdf_entry [ \"@context\" ], str ): if rdf_entry [ \"@context\" ] . startswith ( \"http://schema.org\" ) or rdf_entry [ \"@context\" ] . startswith ( \"https://schema.org\" ): rdf_entry [ \"@context\" ] = \"https://schema.org/docs/jsonldcontext.json\" if isinstance ( rdf_entry [ \"@context\" ], list ): for i , cont in enumerate ( rdf_entry [ \"@context\" ]): if isinstance ( cont , str ): rdf_entry [ \"@context\" ][ i ] = \"https://schema.org/docs/jsonldcontext.json\" except : pass # RDFLib JSON-LD had issue with encoding: https://github.com/RDFLib/rdflib/issues/1416 rdf_data = jsonld . expand ( rdf_data ) rdf_data = json . dumps ( rdf_data ) parse_formats = [ \"json-ld\" ] else : # Try to guess the format to parse from mime type mime_type = mime_type . split ( \";\" )[ 0 ] if \"turtle\" in mime_type : parse_formats = [ \"turtle\" ] elif \"xml\" in mime_type : parse_formats = [ \"xml\" ] elif \"ntriples\" in mime_type : parse_formats = [ \"ntriples\" ] elif \"nquads\" in mime_type : parse_formats = [ \"nquads\" ] elif \"trig\" in mime_type : parse_formats = [ \"trig\" ] # elif mime_type.startswith('text/html'): # parse_formats = [] g = ConjunctiveGraph () # Remove some auto-generated triples about the HTML content remove_preds = [ \"http://www.w3.org/1999/xhtml/vocab#role\" ] for rdf_format in parse_formats : try : g = ConjunctiveGraph () g . parse ( data = rdf_data , format = rdf_format ) for rm_pred in remove_preds : g . remove (( None , URIRef ( rm_pred ), None )) self . info ( f \"Successfully parsed { mime_type } RDF from { log_msg } with parser { rdf_format } , containing { str ( len ( g )) } triples\" ) return g except Exception as e : self . info ( f \"Could not parse { mime_type } metadata from { log_msg } with parser { rdf_format } . Getting error: { str ( e ) } \" ) return g","title":"parse_rdf()"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.response","text":"Function used to generate the FAIR metric test results as JSON-LD, and return this JSON-LD as HTTP response Returns: Name Type Description response JSONResponse HTTP response containing the test results as JSON-LD Source code in fair_test/fair_test_evaluation.py 509 510 511 512 513 514 515 516 def response ( self ) -> JSONResponse : \"\"\" Function used to generate the FAIR metric test results as JSON-LD, and return this JSON-LD as HTTP response Returns: response: HTTP response containing the test results as JSON-LD \"\"\" return JSONResponse ( self . to_jsonld ())","title":"response()"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.retrieve_metadata","text":"Retrieve metadata from a URL, RDF metadata parsed as a RDFLib Graph in priority. Super useful. It tries: - Following signposting links (returned in HTTP headers) - Extracting JSON-LD embedded in the HTML - Asking RDF through content-negociation - Can return JSON found as a fallback, if RDF metadata is not found You can also use an external harvester API to get the RDF metadata Parameters: Name Type Description Default url str URL to retrieve RDF from required use_harvester Optional [ bool ] Use an external harvester to retrieve the RDF instead of the built-in python harvester False harvester_url Optional [ str ] URL of the RDF harvester used 'https://fair-tests.137.120.31.101.nip.io/tests/harvester' Returns: Name Type Description g Graph A RDFLib Graph with the RDF found at the given URL Source code in fair_test/fair_test_evaluation.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 def retrieve_metadata ( self , url : str , use_harvester : Optional [ bool ] = False , harvester_url : Optional [ str ] = \"https://fair-tests.137.120.31.101.nip.io/tests/harvester\" , ) -> Any : \"\"\" Retrieve metadata from a URL, RDF metadata parsed as a RDFLib Graph in priority. Super useful. It tries: - Following signposting links (returned in HTTP headers) - Extracting JSON-LD embedded in the HTML - Asking RDF through content-negociation - Can return JSON found as a fallback, if RDF metadata is not found You can also use an external harvester API to get the RDF metadata Parameters: url: URL to retrieve RDF from use_harvester: Use an external harvester to retrieve the RDF instead of the built-in python harvester harvester_url: URL of the RDF harvester used Returns: g (Graph): A RDFLib Graph with the RDF found at the given URL \"\"\" original_url = url url = self . get_url ( url ) if not url : self . warn ( f \"The resource { original_url } could not be converted to a valid URL, hence no metadata could be retrieved\" ) return [] if use_harvester == True : # Check the harvester response: # curl -X POST -d '{\"subject\": \"https://doi.org/10.1594/PANGAEA.908011\"}' https://fair-tests.137.120.31.101.nip.io/tests/harvester try : self . info ( f \"Using Harvester at { harvester_url } to retrieve RDF metadata at { url } \" ) res = requests . post ( harvester_url , json = { \"subject\" : url }, timeout = 60 , # headers={\"Accept\": \"application/ld+json\"} ) return self . parse_rdf ( res . text , \"text/turtle\" , log_msg = \"FAIR evaluator harvester RDF\" ) except Exception : self . warn ( f \"Failed to reach the Harvester at { harvester_url } , using the built-in python harvester\" ) # https://github.com/FAIRMetrics/Metrics/blob/master/MetricsEvaluatorCode/Ruby/metrictests/fair_metrics_utilities.rb#L355 html_text = None metadata_obj = [] # Check if URL resolve and if redirection # r = requests.head(url) try : r = requests . get ( url ) r . raise_for_status () # Raises a HTTPError if the status is 4xx, 5xxx self . info ( f \"Successfully resolved { url } \" ) html_text = r . text if r . history : # Extract alternative URIs if request redirected redirect_url = r . url if redirect_url . startswith ( \"https://linkinghub.elsevier.com/retrieve/pii/\" ): # Special case to handle Elsevier bad redirections to ScienceDirect redirect_url = redirect_url . replace ( \"https://linkinghub.elsevier.com/retrieve/pii/\" , \"https://www.sciencedirect.com/science/article/pii/\" , ) self . data [ \"redirect_url\" ] = redirect_url if url == self . subject and not redirect_url in self . data [ \"alternative_uris\" ]: self . info ( f \"Request was redirected to { redirect_url } , adding to the list of alternative URIs for the subject\" ) self . data [ \"alternative_uris\" ] . append ( redirect_url ) if r . url . startswith ( \"http://\" ): self . data [ \"alternative_uris\" ] . append ( redirect_url . replace ( \"http://\" , \"https://\" )) elif r . url . startswith ( \"https://\" ): self . data [ \"alternative_uris\" ] . append ( redirect_url . replace ( \"https://\" , \"http://\" )) # Handle signposting links headers https://signposting.org/FAIR if r . links : # Follow signposting links, this could create a lot of recursions (to be checked) self . info ( f \"Found Signposting links: { str ( r . links ) } \" ) self . data [ \"signposting_links\" ] = r . links check_rels = [ \"alternate\" , \"describedby\" , \"meta\" ] # Alternate is used by schema.org for rel in check_rels : if rel in r . links . keys (): rel_url = r . links [ rel ][ \"url\" ] if not rel_url . startswith ( \"http://\" ) and not rel_url . startswith ( \"https://\" ): # In some case the rel URL provided is relative to the requested URL if r . url . endswith ( \"/\" ) and rel_url . startswith ( \"/\" ): rel_url = rel_url [ 1 :] rel_url = r . url + rel_url metadata_obj = self . retrieve_metadata ( rel_url ) if len ( metadata_obj ) > 0 : return metadata_obj except Exception as e : self . warn ( f \"Error resolving the URL { url } : { str ( e . args [ 0 ]) } \" ) self . info ( \"Checking for metadata embedded in the HTML page returned by the resource URI \" + url + \" using extruct\" ) # TODO: support client-side JS generated HTML using Selenium https://github.com/vemonet/extruct-selenium try : extructed = extruct . extract ( html_text . encode ( \"utf8\" )) if url == self . subject : self . data [ \"extruct\" ] = extructed if len ( extructed [ \"json-ld\" ]) > 0 : g = self . parse_rdf ( extructed [ \"json-ld\" ], \"json-ld\" , log_msg = \"HTML embedded JSON-LD RDF\" ) if len ( g ) > 0 : self . info ( f \"Found JSON-LD RDF metadata embedded in the HTML with extruct\" ) return g else : metadata_obj = extructed [ \"json-ld\" ] if len ( extructed [ \"rdfa\" ]) > 0 : g = self . parse_rdf ( extructed [ \"rdfa\" ], \"json-ld\" , log_msg = \"HTML embedded RDFa\" ) if len ( g ) > 0 : self . info ( f \"Found RDFa metadata embedded in the HTML with extruct\" ) return g elif not metadata_obj : metadata_obj = extructed [ \"rdfa\" ] if not metadata_obj and len ( extructed [ \"microdata\" ]) > 0 : metadata_obj = extructed [ \"microdata\" ] if not metadata_obj and extructed [ \"dublincore\" ] != [{ \"namespaces\" : {}, \"elements\" : [], \"terms\" : []}]: # Dublin core always comes as this empty dict if no match metadata_obj = extructed [ \"dublincore\" ] # The rest is not extracted because usually give no interesting metadata: # opengraph, microformat except Exception as e : self . info ( f \"Error when running extruct on { url } . Getting: { str ( e . args [ 0 ]) } \" ) # Perform content negociation last because it's the slowest for a lot of URLs like zenodo # We need to do direct content negociation to turtle and json # because some URLs dont support standard weighted content negociation check_mime_types = [ \"text/turtle\" , \"application/ld+json\" , \"text/turtle, application/turtle, application/x-turtle;q=0.9, application/ld+json;q=0.8, application/rdf+xml, text/n3, text/rdf+n3;q=0.7\" , ] for mime_type in check_mime_types : try : r = requests . get ( url , headers = { \"accept\" : mime_type }) r . raise_for_status () # Raises a HTTPError if the status is 4xx, 5xxx content_type = r . headers [ \"Content-Type\" ] . replace ( \" \" , \"\" ) . replace ( \";charset=utf-8\" , \"\" ) # If return text/plain we parse as turtle or JSON-LD # content_type = content_type.replace('text/plain', 'text/turtle') self . info ( f \"Content-negotiation: found some metadata in { content_type } when asking for { mime_type } \" ) try : # If returns JSON self . data [ \"json-ld\" ] = r . json () if not metadata_obj : metadata_obj = r . json () return self . parse_rdf ( r . json (), \"json-ld\" , log_msg = \"content negotiation JSON-LD RDF\" ) except Exception : # If returns RDF as text, such as turtle return self . parse_rdf ( r . text , content_type , log_msg = \"content negotiation RDF\" ) except Exception as e : self . info ( f \"Content-negotiation: error with { url } when asking for { mime_type } . Getting { str ( e . args [ 0 ]) } \" ) # Error: e.args[0] return metadata_obj","title":"retrieve_metadata()"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.success","text":"Log a success message related to the FAIR test execution (add to the comments of the test and set score to 1) Parameters: Name Type Description Default log_msg str Message to log required Source code in fair_test/fair_test_evaluation.py 586 587 588 589 590 591 592 593 594 595 596 597 def success ( self , log_msg : str ) -> None : \"\"\" Log a success message related to the FAIR test execution (add to the comments of the test and set score to 1) Parameters: log_msg: Message to log \"\"\" if self . score >= 1 : self . bonus ( log_msg ) else : self . score += 1 self . log ( log_msg , \"SUCCESS:\" )","title":"success()"},{"location":"FairTestEvaluation/#fair_test.fair_test_evaluation.FairTestEvaluation.warn","text":"Log a warning related to the FAIR test execution (add to the comments of the test) Parameters: Name Type Description Default log_msg str Message to log required Source code in fair_test/fair_test_evaluation.py 558 559 560 561 562 563 564 565 def warn ( self , log_msg : str ) -> None : \"\"\" Log a warning related to the FAIR test execution (add to the comments of the test) Parameters: log_msg: Message to log \"\"\" self . log ( log_msg , \"WARN:\" )","title":"warn()"},{"location":"development/","text":"\ud83d\udce5 Install for development Clone the repository and go in the project folder: git clone https://github.com/MaastrichtU-IDS/fair-test cd fair-test To install the project for development you can either use venv to create a virtual environment yourself, or use hatch to automatically handle virtual environments for you. venv hatch Create the virtual environment in the project folder : python3 -m venv .venv Activate the virtual environment: source .venv/bin/activate Install all dependencies required for development: pip install -e \".[dev,doc,test]\" You can also enable automated formatting of the code at each commit: pre-commit install Install Hatch , this will automatically handle virtual environments and make sure all dependencies are installed when you run a script in the project: pip install hatch Optionally you can improve hatch terminal completion See the official documentation for more details. For ZSH you can run these commands: _HATCH_COMPLETE = zsh_source hatch > ~/.hatch-complete.zsh echo \". ~/.hatch-complete.zsh\" >> ~/.zshrc \ud83e\uddd1\u200d\ud83d\udcbb Development workflow venv hatch Deploy the FAIR test API defined in the example folder to test your changes: ./scripts/dev.sh The code will be automatically formatted when you commit your changes using pre-commit . But you can also run the script to format the code yourself: ./scripts/format.sh Or check the code for errors: ./scripts/lint.sh Deploy the FAIR test API defined in the example folder to test your changes: hatch run dev The code will be automatically formatted when you commit your changes using pre-commit . But you can also run the script to format the code yourself: hatch run format Or check the code for errors: hatch run lint \u2705 Run the tests Tests are automatically run by a GitHub Actions workflow when new code is pushed to the GitHub repository. The subject URLs to test and their expected score are retrieved from the test_test attribute for each metric test.??? success \u201cInstall pytest for testing\u201d If not already done, define the 2 files required to run the tests It will test all cases defined in your FAIR metrics tests test_test attributes: tests/conftest.py def pytest_addoption ( parser ): parser . addoption ( \"--metric\" , action = \"store\" , default = None ) and: tests/test_metrics.py import pytest from fastapi.testclient import TestClient from main import app endpoint = TestClient ( app ) def test_api ( pytestconfig ): app . run_tests ( endpoint , pytestconfig . getoption ( 'metric' )) venv hatch Run the tests locally: ./scripts/test.sh You can also run the tests only for a specific metric test: ./scripts/test.sh --metric a1-metadata-protocol Run the tests locally: hatch run test \ud83d\udcd6 Generate docs The documentation (this website) is automatically generated from the markdown files in the docs folder and python docstring comments, and published by a GitHub Actions workflow. Serve the docs on http://localhost:8008 venv hatch ./scripts/docs-serve.sh hatch run docs \ud83c\udff7\ufe0f Publish a new release Increment the __version__ in fair_test/__init__.py Push to GitHub Create a new release on GitHub A GitHub Action workflow will automatically publish the new version to PyPI","title":"Development"},{"location":"development/#install-for-development","text":"Clone the repository and go in the project folder: git clone https://github.com/MaastrichtU-IDS/fair-test cd fair-test To install the project for development you can either use venv to create a virtual environment yourself, or use hatch to automatically handle virtual environments for you. venv hatch Create the virtual environment in the project folder : python3 -m venv .venv Activate the virtual environment: source .venv/bin/activate Install all dependencies required for development: pip install -e \".[dev,doc,test]\" You can also enable automated formatting of the code at each commit: pre-commit install Install Hatch , this will automatically handle virtual environments and make sure all dependencies are installed when you run a script in the project: pip install hatch Optionally you can improve hatch terminal completion See the official documentation for more details. For ZSH you can run these commands: _HATCH_COMPLETE = zsh_source hatch > ~/.hatch-complete.zsh echo \". ~/.hatch-complete.zsh\" >> ~/.zshrc","title":"\ud83d\udce5 Install for development"},{"location":"development/#development-workflow","text":"venv hatch Deploy the FAIR test API defined in the example folder to test your changes: ./scripts/dev.sh The code will be automatically formatted when you commit your changes using pre-commit . But you can also run the script to format the code yourself: ./scripts/format.sh Or check the code for errors: ./scripts/lint.sh Deploy the FAIR test API defined in the example folder to test your changes: hatch run dev The code will be automatically formatted when you commit your changes using pre-commit . But you can also run the script to format the code yourself: hatch run format Or check the code for errors: hatch run lint","title":"\ud83e\uddd1\u200d\ud83d\udcbb Development workflow"},{"location":"development/#run-the-tests","text":"Tests are automatically run by a GitHub Actions workflow when new code is pushed to the GitHub repository. The subject URLs to test and their expected score are retrieved from the test_test attribute for each metric test.??? success \u201cInstall pytest for testing\u201d If not already done, define the 2 files required to run the tests It will test all cases defined in your FAIR metrics tests test_test attributes: tests/conftest.py def pytest_addoption ( parser ): parser . addoption ( \"--metric\" , action = \"store\" , default = None ) and: tests/test_metrics.py import pytest from fastapi.testclient import TestClient from main import app endpoint = TestClient ( app ) def test_api ( pytestconfig ): app . run_tests ( endpoint , pytestconfig . getoption ( 'metric' )) venv hatch Run the tests locally: ./scripts/test.sh You can also run the tests only for a specific metric test: ./scripts/test.sh --metric a1-metadata-protocol Run the tests locally: hatch run test","title":"\u2705 Run the tests"},{"location":"development/#generate-docs","text":"The documentation (this website) is automatically generated from the markdown files in the docs folder and python docstring comments, and published by a GitHub Actions workflow. Serve the docs on http://localhost:8008 venv hatch ./scripts/docs-serve.sh hatch run docs","title":"\ud83d\udcd6 Generate docs"},{"location":"development/#publish-a-new-release","text":"Increment the __version__ in fair_test/__init__.py Push to GitHub Create a new release on GitHub A GitHub Action workflow will automatically publish the new version to PyPI","title":"\ud83c\udff7\ufe0f Publish a new release"},{"location":"publish/","text":"Once your test work as expected it is time to publish and register it, in order to be able to use it in FAIR evaluations services. You can either publish your new tests directly in our existing FAIR enough metrics API, or publish a new API on your servers. \ud83d\uddde\ufe0f First, publish your tests Multiple are available for publishing your tests: \u26a1\ufe0f Publish your tests to an existing API You are welcome to add your tests to the FAIR Enough metrics API. To do so: fork the repository on GitHub, create your metrics tests in the metrics folder, and submit a pull request to propose adding your tests to the FAIR enough metrics API repository , it will be made available at https://metrics.api.fair-enough.semanticscience.org . Persistent URLs This service uses persistent URLs. This means your test will be made permanently available at the URL https://w3id.org/fair-enough/metrics/tests/ + your test path \ud83d\uddc4\ufe0f Publish a new API on your server You can also easily deploy a new API on your servers. If you want to start from a project with everything ready to deploy in production, we recommend you to fork the fair-enough-metrics repository . Change the configuration in main.py and .env , and remove the metrics tests to put yours. See the README for more details on how to deploy with docker-compose \u2601\ufe0f Publish a new API to a cloud provider You can easily publish the docker container running your API using Google Cloud Run , or AWS lambda \ud83d\udccd Then, register your tests Finally, you will need Register your FAIR Metrics Test in a FAIR evaluation service, such as FAIR enough , or the FAIR evaluator , to be able to use it as part of FAIR evaluations. To register your service in FAIR enough: Go to https://fair-enough.semanticscience.org/metrics Provide your publicly available metrics test URL, and click submit. Use persistent URLs When it has been enabled for the FAIR Test API, use your Metrics test persistent URL. Once your FAIR metrics tests are registered in the FAIR evaluation service, you can create collections that use your tests, and run evaluations with those collections.","title":"Publish"},{"location":"publish/#first-publish-your-tests","text":"Multiple are available for publishing your tests:","title":"\ud83d\uddde\ufe0f First, publish your tests"},{"location":"publish/#publish-your-tests-to-an-existing-api","text":"You are welcome to add your tests to the FAIR Enough metrics API. To do so: fork the repository on GitHub, create your metrics tests in the metrics folder, and submit a pull request to propose adding your tests to the FAIR enough metrics API repository , it will be made available at https://metrics.api.fair-enough.semanticscience.org . Persistent URLs This service uses persistent URLs. This means your test will be made permanently available at the URL https://w3id.org/fair-enough/metrics/tests/ + your test path","title":"\u26a1\ufe0f Publish your  tests to an existing API"},{"location":"publish/#publish-a-new-api-on-your-server","text":"You can also easily deploy a new API on your servers. If you want to start from a project with everything ready to deploy in production, we recommend you to fork the fair-enough-metrics repository . Change the configuration in main.py and .env , and remove the metrics tests to put yours. See the README for more details on how to deploy with docker-compose","title":"\ud83d\uddc4\ufe0f Publish a new API on your server"},{"location":"publish/#publish-a-new-api-to-a-cloud-provider","text":"You can easily publish the docker container running your API using Google Cloud Run , or AWS lambda","title":"\u2601\ufe0f Publish a new API to a cloud provider"},{"location":"publish/#then-register-your-tests","text":"Finally, you will need Register your FAIR Metrics Test in a FAIR evaluation service, such as FAIR enough , or the FAIR evaluator , to be able to use it as part of FAIR evaluations. To register your service in FAIR enough: Go to https://fair-enough.semanticscience.org/metrics Provide your publicly available metrics test URL, and click submit. Use persistent URLs When it has been enabled for the FAIR Test API, use your Metrics test persistent URL. Once your FAIR metrics tests are registered in the FAIR evaluation service, you can create collections that use your tests, and run evaluations with those collections.","title":"\ud83d\udccd Then, register your tests"},{"location":"usage/","text":"This page explains how to create a FAIR metrics test API with fair-test . \ud83d\udce5 Install the package Install the package from PyPI : pip install fair-test \ud83d\udcdd Define the API Create a main.py file to declare the API, you can provide a different folder than metrics here, the folder path is relative to where you start the API (the root of the repository): main.py from fair_test import FairTestAPI app = FairTestAPI ( title = 'FAIR Metrics tests API' , metrics_folder_path = 'metrics' , description = \"\"\"FAIR Metrics tests API\"\"\" , cors_enabled = True , license_info = { \"name\" : \"MIT license\" , \"url\" : \"https://opensource.org/licenses/MIT\" }, ) Create a .env file to provide the global informations used for the API, such as contact details and the host URL (note that you don\u2019t need to change it for localhost in development), e.g.: .env HOST_URL = \"https://metrics.api.fair-enough.semanticscience.org\" CONTACT_URL = \"https://github.com/MaastrichtU-IDS/fair-enough-metrics\" CONTACT_NAME = \"Vincent Emonet\" CONTACT_EMAIL = \"vincent.emonet@gmail.com\" CONTACT_ORCID = \"0000-0000-0000-0000\" ORG_NAME = \"Institute of Data Science at Maastricht University\" DEFAULT_SUBJECT = \"https://doi.org/10.1594/PANGAEA.908011\" You can also securely provide secrets environment variables It can be useful to pass API keys to use private services in your metrics tests, such as Search engines APIs. In this example we will define an API key to perform Bing search named APIKEY_BING_SEARCH Create an additional secrets.env environment file, it should not be committed to git (make sure it is added to the .gitignore ). secrets.env APIKEY_BING_SEARCH = yourapikey Add this file to your docker-compose.prod.yml to use the secrets in production: docker-compose.prod.yml services : api : env_file : - secrets.env To use the secret in development you can also add it to the docker-compose.yml , or define the env variable locally in your terminal with export APIKEY_BING_SEARCH=\"yourapikey\" . But be careful not blowing up your quotas. You can then retrieve this API key in your metrics tests: metrics/a1_check_something.py import os apikey = os . getenv ( 'APIKEY_BING_SEARCH' ) \ud83c\udfaf Define a FAIR metrics test Create a a1_check_something.py file in the metrics folder with your test: metrics/a1_check_something.py from fair_test import FairTest , FairTestEvaluation class MetricTest ( FairTest ): metric_path = 'a1-check-something' applies_to_principle = 'A1' title = 'Check something' description = \"\"\"Test something\"\"\" # Optional, infos about contacts will be defined by the .env file if not provided here author = 'https://orcid.org/0000-0000-0000-0000' contact_url = \"https://github.com/LUMC-BioSemantics/RD-FAIRmetrics\" contact_name = \"Your Name\" contact_email = \"your.email@email.com\" organization = \"The Organization for which this test is published\" # Optional, if your metric test has a detailed readme: metric_readme_url = \"https://w3id.org/rd-fairmetrics/RD-F4\" metric_version = '0.1.0' test_test = { 'https://w3id.org/fair-enough/collections' : 1 , 'http://example.com' : 0 , } def evaluate ( self , eval : FairTestEvaluation ): eval . info ( f 'Checking something for { self . subject } ' ) g = eval . retrieve_metadata ( self . subject , use_harvester = False ) if len ( g ) > 0 : eval . success ( f ' { len ( g ) } triples found, test sucessful' ) else : eval . failure ( 'No triples found, test failed' ) return eval . response () \u2139\ufe0f A few common operations are available on the self object: Logging operations: eval . info ( 'Something happened' ) eval . warn ( 'Something bad happened' ) eval . failure ( 'The test failed' ) eval . success ( 'The test succeeded' ) Retrieve RDF from a URL (returns a RDFLib Graph): g = eval . retrieve_metadata ( eval . subject ) Parse a string to RDF: g = eval . parse_rdf ( text , mime_type = 'text/turtle' , log_msg = 'RDF from the subject URL' ) Return the metric test results: return eval . response () There is also a dictionary test_test to define URIs to be automatically tested against each metric, and the expected score. See the Development section for more detail on running the tests. Documentation for all functions You can find the details for all functions available in the Code reference section \ud83e\udd84 Deploy the API You can then run the metrics tests API using uvicorn . Example For example you can get started with the code provided in the example folder , check the example/README.md for more options, such as deploying it with docker. Go to the FAIR metrics API folder, and install the requirements: cd example pip install -r requirements.txt Start the API: uvicorn main:app --reload You can now access the FAIR Test API on http://localhost:8000 and try to run your test using the POST request, or get its descriptive metadata with the GET request.","title":"Usage"},{"location":"usage/#install-the-package","text":"Install the package from PyPI : pip install fair-test","title":"\ud83d\udce5 Install the package"},{"location":"usage/#define-the-api","text":"Create a main.py file to declare the API, you can provide a different folder than metrics here, the folder path is relative to where you start the API (the root of the repository): main.py from fair_test import FairTestAPI app = FairTestAPI ( title = 'FAIR Metrics tests API' , metrics_folder_path = 'metrics' , description = \"\"\"FAIR Metrics tests API\"\"\" , cors_enabled = True , license_info = { \"name\" : \"MIT license\" , \"url\" : \"https://opensource.org/licenses/MIT\" }, ) Create a .env file to provide the global informations used for the API, such as contact details and the host URL (note that you don\u2019t need to change it for localhost in development), e.g.: .env HOST_URL = \"https://metrics.api.fair-enough.semanticscience.org\" CONTACT_URL = \"https://github.com/MaastrichtU-IDS/fair-enough-metrics\" CONTACT_NAME = \"Vincent Emonet\" CONTACT_EMAIL = \"vincent.emonet@gmail.com\" CONTACT_ORCID = \"0000-0000-0000-0000\" ORG_NAME = \"Institute of Data Science at Maastricht University\" DEFAULT_SUBJECT = \"https://doi.org/10.1594/PANGAEA.908011\" You can also securely provide secrets environment variables It can be useful to pass API keys to use private services in your metrics tests, such as Search engines APIs. In this example we will define an API key to perform Bing search named APIKEY_BING_SEARCH Create an additional secrets.env environment file, it should not be committed to git (make sure it is added to the .gitignore ). secrets.env APIKEY_BING_SEARCH = yourapikey Add this file to your docker-compose.prod.yml to use the secrets in production: docker-compose.prod.yml services : api : env_file : - secrets.env To use the secret in development you can also add it to the docker-compose.yml , or define the env variable locally in your terminal with export APIKEY_BING_SEARCH=\"yourapikey\" . But be careful not blowing up your quotas. You can then retrieve this API key in your metrics tests: metrics/a1_check_something.py import os apikey = os . getenv ( 'APIKEY_BING_SEARCH' )","title":"\ud83d\udcdd Define the API"},{"location":"usage/#define-a-fair-metrics-test","text":"Create a a1_check_something.py file in the metrics folder with your test: metrics/a1_check_something.py from fair_test import FairTest , FairTestEvaluation class MetricTest ( FairTest ): metric_path = 'a1-check-something' applies_to_principle = 'A1' title = 'Check something' description = \"\"\"Test something\"\"\" # Optional, infos about contacts will be defined by the .env file if not provided here author = 'https://orcid.org/0000-0000-0000-0000' contact_url = \"https://github.com/LUMC-BioSemantics/RD-FAIRmetrics\" contact_name = \"Your Name\" contact_email = \"your.email@email.com\" organization = \"The Organization for which this test is published\" # Optional, if your metric test has a detailed readme: metric_readme_url = \"https://w3id.org/rd-fairmetrics/RD-F4\" metric_version = '0.1.0' test_test = { 'https://w3id.org/fair-enough/collections' : 1 , 'http://example.com' : 0 , } def evaluate ( self , eval : FairTestEvaluation ): eval . info ( f 'Checking something for { self . subject } ' ) g = eval . retrieve_metadata ( self . subject , use_harvester = False ) if len ( g ) > 0 : eval . success ( f ' { len ( g ) } triples found, test sucessful' ) else : eval . failure ( 'No triples found, test failed' ) return eval . response () \u2139\ufe0f A few common operations are available on the self object: Logging operations: eval . info ( 'Something happened' ) eval . warn ( 'Something bad happened' ) eval . failure ( 'The test failed' ) eval . success ( 'The test succeeded' ) Retrieve RDF from a URL (returns a RDFLib Graph): g = eval . retrieve_metadata ( eval . subject ) Parse a string to RDF: g = eval . parse_rdf ( text , mime_type = 'text/turtle' , log_msg = 'RDF from the subject URL' ) Return the metric test results: return eval . response () There is also a dictionary test_test to define URIs to be automatically tested against each metric, and the expected score. See the Development section for more detail on running the tests. Documentation for all functions You can find the details for all functions available in the Code reference section","title":"\ud83c\udfaf Define a FAIR metrics test"},{"location":"usage/#deploy-the-api","text":"You can then run the metrics tests API using uvicorn . Example For example you can get started with the code provided in the example folder , check the example/README.md for more options, such as deploying it with docker. Go to the FAIR metrics API folder, and install the requirements: cd example pip install -r requirements.txt Start the API: uvicorn main:app --reload You can now access the FAIR Test API on http://localhost:8000 and try to run your test using the POST request, or get its descriptive metadata with the GET request.","title":"\ud83e\udd84 Deploy the API"}]}